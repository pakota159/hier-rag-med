# HierRAGMed

A medical RAG (Retrieval-Augmented Generation) system for local development, built with Python, Ollama, LangChain, ChromaDB, and sentence-transformers.

## Features

- Document processing and chunking for medical texts
- Semantic and hybrid search using ChromaDB
- Local LLM integration with Ollama
- FastAPI web interface
- Comprehensive evaluation metrics
- Configurable system prompts

## Prerequisites

- **Python 3.10+**
- **Conda** (Miniconda or Anaconda)
- **Ollama** for local LLM
- **Cross-platform**: Windows, macOS, Linux

## ğŸš€ Quick Setup

### 1. Install Prerequisites

**Ollama Installation:**
```bash
# macOS
brew install ollama

# Windows
winget install Ollama.Ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Setup Project

```bash
# Clone repository
git clone https://github.com/pakota159/hier-rag-med.git
cd hier-rag-med

# Create conda environment
conda create -n rag-med python=3.10 -y

# Activate environment
conda activate rag-med

# Install dependencies
pip install -r requirements.txt

# Create necessary directories
mkdir -p data/{raw,processed,vector_db,logs}
```

### 3. Setup Ollama Model

```bash
# Start Ollama service
ollama serve &

# Pull the required model
ollama pull mistral:7b-instruct
```

### 4. Run Test

```bash
streamlit run src/simple/streamlit_app.py --server.port 8501
```

## ğŸ“ Complete Project Structure

```
hierragmed/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ simple/              # Basic RAG implementation
â”‚   â”œâ”€â”€ kg/                  # Knowledge Graph enhanced version
â”‚   â”œâ”€â”€ basic_reasoning/     # Foundation reasoning datasets
â”‚   â”œâ”€â”€ evaluation/          # Comprehensive evaluation system
â”‚   â”‚   â”œâ”€â”€ benchmarks/      # MIRAGE, MedReason, PubMedQA, MS MARCO
â”‚   â”‚   â”œâ”€â”€ evaluators/      # KG and Hierarchical system evaluators
â”‚   â”‚   â”œâ”€â”€ metrics/         # QA, retrieval, clinical metrics
â”‚   â”‚   â”œâ”€â”€ utils/           # Visualization, reporting, analysis
â”‚   â”‚   â”œâ”€â”€ configs/         # GPU and local configurations
â”‚   â”‚   â””â”€â”€ streamlit_evaluation.py  # Web evaluation interface
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ processing.py
â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”œâ”€â”€ generation.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original HierRAGMed documents (5K)
â”‚   â”œâ”€â”€ kg_raw/              # KG enhanced data (Phase 1: 95K target)
â”‚   â”œâ”€â”€ foundation_dataset/  # Foundation reasoning datasets
â”‚   â”œâ”€â”€ benchmarks/          # ğŸ“Š Evaluation benchmark datasets (auto-downloaded)
â”‚   â”‚   â”œâ”€â”€ mirage/          # MIRAGE clinical reasoning questions
â”‚   â”‚   â”œâ”€â”€ medreason/       # MedReason knowledge graph reasoning chains
â”‚   â”‚   â”œâ”€â”€ pubmedqa/        # PubMedQA research literature QA
â”‚   â”‚   â””â”€â”€ msmarco/         # MS MARCO passage retrieval queries
â”‚   â”œâ”€â”€ processed/           # Processed documents cache
â”‚   â”œâ”€â”€ vector_db/           # ChromaDB vector storage
â”‚   â””â”€â”€ logs/                # Application logs
â”œâ”€â”€ evaluation/              # ğŸ“ˆ Evaluation outputs (auto-generated)
â”‚   â”œâ”€â”€ results/             # Evaluation results and reports  
â”‚   â”œâ”€â”€ cache/               # Model outputs and processed benchmark cache
â”‚   â””â”€â”€ logs/                # Evaluation run logs and GPU monitoring
â”œâ”€â”€ fetch_data.py            # Data fetching utilities
â”œâ”€â”€ config.yaml              # Main configuration file
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ environment.yml          # Conda environment (optional)
â””â”€â”€ README.md
```

## ğŸ“Š Dataset Storage & Management

### **ğŸ¯ Evaluation Benchmarks (Auto-Downloaded)**

The evaluation system automatically downloads and manages benchmark datasets:

```bash
# Benchmarks are automatically downloaded on first evaluation run
# No manual setup required!

data/benchmarks/
â”œâ”€â”€ mirage/          # ~10K clinical reasoning questions
â”œâ”€â”€ medreason/       # ~32K knowledge graph reasoning chains  
â”œâ”€â”€ pubmedqa/        # ~1K expert-annotated QA pairs
â””â”€â”€ msmarco/         # ~100K passage retrieval queries
```

**Sources & Auto-Download:**
- **MIRAGE:** Hugging Face `mirage_benchmark` - Clinical reasoning scenarios
- **MedReason:** Hugging Face `UCSC-VLAA/MedReason` - KG-guided reasoning chains
- **PubMedQA:** Hugging Face `pubmed_qa` - Research literature Q&A
- **MS MARCO:** Microsoft research - Passage retrieval benchmark

### **ğŸ—ï¸ Training Data (Manual Collection)**

```bash
# Training data for HierRAGMed system
data/
â”œâ”€â”€ raw/                    # Original documents (~5K)
â”œâ”€â”€ kg_raw/                # Knowledge graph data (~95K target)
â””â”€â”€ foundation_dataset/    # Foundation reasoning datasets
```

**Manual Collection Commands:**
```bash
# Fetch Phase 1 datasets (Target: 95K documents)
python fetch_data.py --source all --max-results 1000

# Fetch specific sources
python fetch_data.py --source pubmed --max-results 500
python fetch_data.py --source mtsamples
python fetch_data.py --source mesh
```

### **ğŸ“ˆ Evaluation Outputs (Auto-Generated)**

```bash
# Created automatically during evaluation - DO NOT commit to git
evaluation/
â”œâ”€â”€ results/        # JSON results, HTML reports, visualizations
â”œâ”€â”€ cache/          # Model outputs cache, processed benchmarks
â””â”€â”€ logs/           # Evaluation logs, GPU monitoring, debug info
```

## ğŸ”¬ Evaluation System

### **Quick Evaluation**
```bash
# Web interface
streamlit run src/evaluation/streamlit_evaluation.py --server.port 8501

# Command line
python src/evaluation/run_evaluation.py

# Model comparison  
python src/evaluation/compare_models.py
```

### **GPU Evaluation (RunPod)**
```bash
# Setup RunPod environment
bash src/evaluation/runpod_setup.sh

# Start GPU evaluation
./start_evaluation.sh

# Monitor GPU usage
./monitor_gpu.sh
```

### **Supported Benchmarks**
- **MIRAGE:** Clinical reasoning and decision-making
- **MedReason:** Knowledge graph-guided medical reasoning  
- **PubMedQA:** Research literature comprehension
- **MS MARCO:** Medical passage retrieval quality

### **Model Systems**
- **KG System:** Knowledge graph enhanced RAG
- **Hierarchical System:** Three-tier diagnostic reasoning
- **Comparative Analysis:** Statistical significance testing

## ğŸ“Š Dataset Expansion for Hierarchical Diagnostic Reasoning

HierRAGMed includes two expansion phases to scale from basic RAG to production-ready hierarchical diagnostic reasoning:

### **Phase 1: Foundation Dataset (Target: ~95K documents)**
**Timeline**: 2-3 weeks | **Status**: Ready for implementation

| Dataset | Size | Type | Purpose | Implementation |
|---------|------|------|---------|----------------|
| **Current Base** | 5.1K | PubMed + MTSamples + MeSH | Medical knowledge foundation | âœ… Complete |
| **MedReason** | 32K | Structured reasoning chains | Diagnostic thinking patterns | ğŸ“‹ Priority 1 |
| **MSDiagnosis** | Variable | Multi-step diagnosis | Three-tier reasoning (Primaryâ†’Differentialâ†’Final) | ğŸ“‹ Priority 1 |
| **PMC-Patients** | 50K | Patient case studies | Clinical reasoning examples | ğŸ“‹ Priority 2 |
| **Drug Database** | 5K | Medication information | Treatment knowledge | ğŸ“‹ Priority 3 |

**Expected Performance**: 68-72% medical QA accuracy, enables basic hierarchical reasoning training

#### **Key Datasets Details**

**ğŸ§  MedReason Dataset (2025)**
- **Innovation**: Knowledge graph-guided reasoning chains
- **Quality**: Validated by medical professionals across specialties
- **Format**: Question-answer pairs with step-by-step explanations
- **Clinical Value**: Provides structured "thinking paths" for medical problem-solving
- **Access**: Open source - https://github.com/UCSC-VLAA/MedReason

**ğŸ¥ MSDiagnosis Dataset (2024)**
- **Innovation**: EMR-based multi-step diagnostic scenarios
- **Quality**: Professional medical team annotation with three-round validation
- **Format**: Primary diagnosis â†’ Differential diagnosis â†’ Final diagnosis
- **Clinical Value**: Directly matches real clinical diagnostic workflows
- **Access**: Research dataset (requires academic access)

**ğŸ“š PMC-Patients Dataset**
- **Innovation**: Comprehensive patient cases from medical literature
- **Quality**: Published medical case studies with peer review
- **Format**: Structured patient summaries with outcomes
- **Clinical Value**: Covers rare diseases and complex diagnostic scenarios
- **Access**: Open source with 167K total patient cases

### **Phase 2: Production Dataset (Target: ~250K+ documents)**
**Timeline**: 3-4 months | **Status**: Future expansion

| Dataset | Size | Type | Purpose | Access |
|---------|------|------|---------|--------|
| **MIMIC-IV** | 100K+ | Real clinical notes | Authentic clinical reasoning | Requires training |
| **Clinical Guidelines** | 10K | WHO/CDC/AHA protocols | Evidence-based medicine | Public APIs |
| **Medical Textbooks** | 20K+ | Harrison's/UpToDate excerpts | Comprehensive medical knowledge | License required |
| **Drug Interactions** | 15K | DrugBank + RxNorm | Pharmacology reasoning | Open research |
| **International Data** | 25K+ | Global medical standards | Diverse clinical practices | Various sources |

**Expected Performance**: 75-78% medical QA accuracy, professional-grade diagnostic reasoning

### **Three-Tier Architecture Data Mapping**

#### **Tier 1: Pattern Recognition (Fast Hypothesis Generation)**
- **MeSH concepts**: Quick symptom-disease associations
- **ICD-10 codes**: Rapid disease classification  
- **Drug databases**: Medication identification
- **Clinical keywords**: Symptom pattern matching

#### **Tier 2: Hypothesis Testing (Systematic Evidence Collection)**
- **MedReason chains**: Structured diagnostic reasoning
- **MSDiagnosis**: Multi-step diagnostic validation
- **Clinical guidelines**: Evidence-based protocols
- **PMC-Patients**: Differential diagnosis examples

#### **Tier 3: Confirmation (Comprehensive Verification)**
- **MIMIC-IV discharge summaries**: Final diagnosis confirmation
- **Medical textbooks**: Authoritative knowledge verification
- **Drug interaction databases**: Treatment safety confirmation
- **International guidelines**: Cross-validation of standards

## ğŸŒ Platform-Specific Notes

### Windows
```powershell
# Activate environment
conda activate rag-med

# Start Ollama service (runs as Windows Service)
ollama serve

# In new PowerShell window
ollama pull mistral:7b-instruct

# Run application
python -m src.main
```

### macOS/Linux
```bash
# Start Ollama in background
ollama serve &

# Pull model
ollama pull mistral:7b-instruct

# Run application
python -m src.main
```

## âš™ï¸ Configuration

The system is configured through `config.yaml`. Key configurations include:

- Model settings (embedding and LLM)
- Retrieval parameters
- Document processing options
- System prompts
- Web interface settings

## ğŸ“š Usage Examples

### 1. Basic Setup (Current: 5K documents)
```bash
# Run simple version
streamlit run src/simple/streamlit_app.py --server.port 8501
```

### 2. Fetch Phase 1 Datasets (Target: 95K documents)
```bash
# Fetch comprehensive medical datasets
python fetch_data.py --source all --max-results 1000

# Run KG-enhanced version
streamlit run src/kg/streamlit_app.py --server.port 8502
```

### 3. Run Foundation Reasoning
```bash
# Run basic reasoning system
streamlit run src/basic_reasoning/streamlit_app.py --server.port 8503
```

### 4. Process Documents
```python
from src import Config, DocumentProcessor

config = Config()
processor = DocumentProcessor(config["processing"])

# Process a single PDF
documents = processor.process_pdf("path/to/document.pdf")

# Process a directory of PDFs
documents = processor.process_directory("path/to/documents/")

# Save processed documents
processor.save_documents(documents, "data/processed/documents.json")
```

### 5. Create Vector Store
```python
from src import Retriever

retriever = Retriever(config)
retriever.create_collection("medical_docs")
retriever.add_documents(documents)
```

### 6. Query the System
```python
from src import Generator

generator = Generator(config)

# Simple query
results = retriever.search("What are the symptoms of diabetes?")
answer = generator.generate("What are the symptoms of diabetes?", results)

# Query with citations
results = retriever.hybrid_search("What are the symptoms of diabetes?")
response = generator.generate_with_citations("What are the symptoms of diabetes?", results)
```

### 7. Web Interface
```bash
# Start the web interface
python -m src.main

# Access API documentation
# http://localhost:8000/docs

# Health check
# http://localhost:8000/health
```

## ğŸ”Œ API Endpoints

- **POST /query**: Submit a query to the RAG system
- **GET /collections**: List available document collections
- **GET /health**: Health check endpoint
- **GET /docs**: Interactive API documentation

## ğŸ“Š Evaluation

The system includes comprehensive evaluation metrics:

- **Retrieval metrics**: precision, recall, F1, semantic similarity
- **Generation metrics**: ROUGE scores, semantic similarity

```python
from src import Evaluator

evaluator = Evaluator(config)
metrics = evaluator.evaluate_rag(
    query="What are the symptoms of diabetes?",
    retrieved_docs=results,
    generated_text=answer,
    reference_docs=reference_docs,
    reference_text=reference_answer
)
```

## ğŸ”§ Environment Management

### Activate Environment
```bash
conda activate rag-med
```

### Update Dependencies
```bash
conda activate rag-med
pip install -r requirements.txt --upgrade
```

### Deactivate Environment
```bash
conda deactivate
```

### Remove Environment
```bash
conda env remove -n rag-med
```

## ğŸš¨ Troubleshooting

### Common Issues

**1. Ollama Connection Error**
```bash
# Make sure Ollama is running
ollama serve

# Check if model is available
ollama list

# Pull model if missing
ollama pull mistral:7b-instruct
```

**2. Import Errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```

**3. ChromaDB Permission Issues**
```bash
# Fix directory permissions
chmod -R 755 data/vector_db/
```

**4. Evaluation Dataset Download Issues**
```bash
# Clear cache and retry
rm -rf evaluation/cache/
python src/evaluation/run_evaluation.py
```

**5. GPU Memory Issues (RunPod)**
```bash
# Monitor GPU usage
./monitor_gpu.sh

# Reduce batch sizes in config
nano src/evaluation/configs/gpu_runpod_config.yaml
```

## ğŸ¯ Key Features

- **ğŸ” Multi-tier Retrieval**: Pattern recognition â†’ Evidence collection â†’ Confirmation
- **ğŸ§  Medical Reasoning**: Knowledge graph enhanced diagnostic thinking
- **ğŸ“Š Comprehensive Evaluation**: 4 medical benchmarks with GPU optimization
- **ğŸš€ Scalable Architecture**: From 5K to 250K+ document capability
- **ğŸŒ Cross-platform**: Windows, macOS, Linux support
- **âš¡ GPU Acceleration**: RunPod optimized for RTX 4090
- **ğŸ“ˆ Performance Tracking**: Real-time evaluation monitoring
- **ğŸ”§ Flexible Configuration**: Local development to production deployment

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.