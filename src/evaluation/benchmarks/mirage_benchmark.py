# Fixed MIRAGE Benchmark Integration
# src/evaluation/benchmarks/mirage_benchmark.py

"""
CORRECTED MIRAGE Benchmark with Official QADataset Integration
Uses src.utils.QADataset from the official MIRAGE repository
"""

import re
import json
import sys
import numpy as np
import requests
from pathlib import Path
from typing import Dict, List, Any, Set, Optional

from .base_benchmark import BaseBenchmark
from loguru import logger

# Add project root to path for proper imports
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# Import official MIRAGE QADataset utility
try:
    from mirage.src.utils import QADataset
    MIRAGE_UTILS_AVAILABLE = True
    logger.info("‚úÖ Successfully imported QADataset from MIRAGE")
except ImportError as e:
    MIRAGE_UTILS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è QADataset from MIRAGE not available: {e} - using fallback data loader")

from src.evaluation.data.data_loader import BenchmarkDataLoader


class MIRAGEBenchmark(BaseBenchmark):
    """
    MIRAGE benchmark with official QADataset integration.
    Uses the official QADataset from MIRAGE repository when available.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.name = "MIRAGE"
        
        # Initialize evaluation models first
        self.similarity_model = None
        self._init_evaluation_models()
        
        # Only initialize data_loader if needed
        try:
            from src.evaluation.data.data_loader import BenchmarkDataLoader
            self.data_loader = BenchmarkDataLoader(config)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not initialize BenchmarkDataLoader: {e}")
            self.data_loader = None
        
        # MIRAGE-specific configuration
        self.use_official_dataset = config.get("use_official_dataset", True)
        self.dataset_source = config.get("dataset_source", "https://github.com/Teddy-XiongGZ/MIRAGE")
        
        # Available MIRAGE datasets
        self.available_datasets = ["mmlu", "medqa", "medmcqa", "pubmedqa", "bioasq"]
        
        # Medical patterns for enhanced evaluation
        self.medical_patterns = {
            'symptoms': ['pain', 'fever', 'nausea', 'vomiting', 'diarrhea', 'fatigue', 'headache', 'shortness of breath'],
            'conditions': ['diabetes', 'hypertension', 'heart failure', 'stroke', 'asthma', 'cancer', 'pneumonia'],
            'treatments': ['medication', 'therapy', 'treatment', 'management', 'intervention', 'surgery', 'procedure'],
            'investigations': ['blood test', 'x-ray', 'ct scan', 'mri', 'ecg', 'examination', 'biopsy', 'ultrasound'],
            'anatomy': ['heart', 'lung', 'liver', 'kidney', 'brain', 'stomach', 'intestine', 'blood vessel']
        }
        
        self.reasoning_indicators = {
            'systematic': ['systematic', 'step-by-step', 'approach', 'method', 'process', 'algorithm'],
            'differential': ['differential', 'consider', 'rule out', 'exclude', 'possible', 'likely', 'unlikely'],
            'evidence': ['evidence', 'studies', 'research', 'guidelines', 'literature', 'recommendation'],
            'causality': ['because', 'due to', 'caused by', 'leads to', 'results in', 'associated with']
        }
    
    def load_dataset(self) -> List[Dict]:
        """Load official MIRAGE dataset using QADataset utility."""
        logger.info(f"üîÑ Loading official MIRAGE benchmark...")
        
        try:
            # Try to use official QADataset first
            if MIRAGE_UTILS_AVAILABLE and self.use_official_dataset:
                return self._load_with_official_qadataset()
            else:
                # Fallback to centralized data loader
                return self._load_with_fallback_loader()
                
        except Exception as e:
            logger.error(f"‚ùå Failed to load MIRAGE data: {e}")
            raise e
    
    def _load_with_official_qadataset(self) -> List[Dict]:
        """Load data using the official QADataset from MIRAGE."""
        logger.info("üìö Using official MIRAGE QADataset...")
        
        all_data = []
        
        try:
            # Check if benchmark.json exists in mirage directory
            mirage_dir = Path("mirage")
            benchmark_file = mirage_dir / "benchmark.json"
            
            if not benchmark_file.exists():
                logger.warning(f"‚ö†Ô∏è MIRAGE benchmark.json not found at {benchmark_file}")
                logger.info("   üí° Try: cd mirage && wget https://github.com/Teddy-XiongGZ/MIRAGE/raw/main/benchmark.json")
                return self._load_with_fallback_loader()
            
            # Change to mirage directory for QADataset to work properly
            import os
            original_cwd = os.getcwd()
            
            try:
                os.chdir(mirage_dir)
                logger.info(f"   üìÅ Changed to MIRAGE directory: {mirage_dir.absolute()}")
                
                # Load all available datasets in MIRAGE
                for dataset_name in self.available_datasets:
                    logger.info(f"   üìñ Loading {dataset_name} dataset...")
                    
                    try:
                        # CORRECTED: Use only dataset_name parameter as per official docs
                        qa_dataset = QADataset(dataset_name)
                        
                        # Load the dataset
                        dataset_length = len(qa_dataset)
                        logger.info(f"   üìã Found {dataset_length} questions in {dataset_name}")
                        
                        # Apply sample limit if specified
                        max_samples = self.sample_size if self.sample_size and not self.is_unlimited else dataset_length
                        actual_samples = min(max_samples, dataset_length)
                        
                        # Process each item
                        for idx in range(actual_samples):
                            item = qa_dataset[idx]
                            processed_item = self._process_mirage_item(item, f"{dataset_name}_{idx}", dataset_name)
                            all_data.append(processed_item)
                        
                        logger.info(f"   ‚úÖ Loaded {actual_samples} questions from {dataset_name}")
                        
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è Failed to load {dataset_name}: {e}")
                        continue
                
            finally:
                # Always change back to original directory
                os.chdir(original_cwd)
                logger.debug(f"   üìÅ Changed back to: {original_cwd}")
            
            if all_data:
                logger.info(f"‚úÖ Total loaded: {len(all_data)} MIRAGE questions using official QADataset")
                return all_data
            else:
                logger.warning("‚ö†Ô∏è No data loaded from official QADataset, falling back...")
                return self._load_with_fallback_loader()
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Official QADataset failed: {e}. Falling back to manual loader.")
            return self._load_with_fallback_loader()
    
    def _load_with_fallback_loader(self) -> List[Dict]:
        """Fallback to centralized data loader or create test data."""
        logger.info("üìö Using fallback MIRAGE data loader...")
        
        if self.data_loader:
            try:
                max_samples = self.sample_size if not self.is_unlimited else None
                data = self.data_loader.load_benchmark_data("mirage", max_samples=max_samples)
                
                if data and len(data) > 0:
                    logger.info(f"‚úÖ Loaded {len(data)} MIRAGE questions from fallback loader")
                    return data
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Fallback loader failed: {e}")
        
        # Create minimal test data if all else fails
        logger.info("üìù Creating minimal test data for MIRAGE benchmark...")
        test_data = []
        
        sample_questions = [
            {
                "question": "A 65-year-old patient presents with chest pain and shortness of breath. What is the most likely diagnosis?",
                "options": {"A": "Myocardial infarction", "B": "Pneumonia", "C": "Anxiety", "D": "Gastroesophageal reflux"},
                "answer": "A"
            },
            {
                "question": "Which of the following is a common side effect of ACE inhibitors?",
                "options": {"A": "Dry cough", "B": "Weight gain", "C": "Hair loss", "D": "Muscle cramps"},
                "answer": "A"
            },
            {
                "question": "What is the most appropriate initial treatment for acute myocardial infarction?",
                "options": {"A": "Aspirin and clopidogrel", "B": "Beta-blockers only", "C": "Calcium channel blockers", "D": "ACE inhibitors"},
                "answer": "A"
            },
            {
                "question": "A patient with diabetes presents with polyuria and polydipsia. What is the most likely cause?",
                "options": {"A": "Hyperglycemia", "B": "Hypoglycemia", "C": "Dehydration", "D": "Kidney disease"},
                "answer": "A"
            },
            {
                "question": "Which medication is first-line treatment for hypertension in most patients?",
                "options": {"A": "ACE inhibitor or ARB", "B": "Beta-blocker", "C": "Calcium channel blocker", "D": "Diuretic"},
                "answer": "A"
            }
        ]
        
        # Extend with more questions if needed
        while len(sample_questions) < (self.sample_size or 10):
            # Duplicate questions with slight variations for testing
            base_q = sample_questions[len(sample_questions) % len(sample_questions)]
            sample_questions.append({
                **base_q,
                "question": f"[Variant] {base_q['question']}"
            })
        
        for idx, item in enumerate(sample_questions):
            if len(test_data) >= (self.sample_size or 10):
                break
                
            processed_item = self._process_mirage_item(item, f"test_{idx}", "test")
            test_data.append(processed_item)
        
        logger.info(f"‚úÖ Created {len(test_data)} test MIRAGE questions")
        return test_data
    
    def _process_mirage_item(self, item: Dict, question_id: str, dataset_name: str) -> Dict:
        """Process a single MIRAGE item into standardized format."""
        # Extract question and options
        question = item.get("question", "")
        options = item.get("options", {})
        answer = item.get("answer", "")
        
        # Convert options to list format if it's a dict
        if isinstance(options, dict):
            options_list = [f"{key}: {value}" for key, value in options.items()]
            options_text = "\n".join(options_list)
        else:
            options_list = options if isinstance(options, list) else []
            options_text = "\n".join(options_list) if options_list else ""
        
        # Determine question type based on options
        if len(options_list) >= 2:
            question_type = "multiple_choice"
        else:
            question_type = "yes_no" if dataset_name in ["pubmedqa", "bioasq"] else "open_ended"
        
        # Map dataset to medical specialty
        specialty_mapping = {
            "mmlu": "general_medicine",
            "medqa": "clinical_medicine", 
            "medmcqa": "clinical_medicine",
            "pubmedqa": "biomedical_research",
            "bioasq": "biomedical_research"
        }
        
        processed_item = {
            "question_id": question_id,
            "question": question,
            "context": "",  # MIRAGE doesn't provide context initially
            "answer": answer,
            "options": options_list,
            "options_text": options_text,
            "question_type": question_type,
            "medical_specialty": specialty_mapping.get(dataset_name, "general_medicine"),
            "reasoning_type": "clinical_reasoning" if dataset_name in ["medqa", "medmcqa"] else "research_analysis",
            "difficulty": self._estimate_difficulty(question, options_list),
            "benchmark": "mirage",
            "source_dataset": dataset_name,
            "source": "official_qadataset"
        }
        
        return processed_item
    
    def _estimate_difficulty(self, question: str, options: List[str]) -> str:
        """Estimate difficulty based on question and options length."""
        question_length = len(question.split())
        options_count = len(options)
        
        if question_length > 100 or options_count > 4:
            return "hard"
        elif question_length > 50 or options_count == 4:
            return "medium"
        else:
            return "easy"
    
    def evaluate_response(self, question: Dict, response: str, retrieved_docs: List[Dict]) -> Dict:
        """Evaluate a model response against MIRAGE ground truth."""
        logger.debug(f"üîç Evaluating MIRAGE response for question {question.get('question_id', 'unknown')}")
        
        metrics = {}
        
        try:
            # 1. Basic accuracy check
            metrics["answer_accuracy"] = self._evaluate_answer_accuracy(response, question)
            
            # 2. Medical terminology usage
            metrics["medical_terminology"] = self._evaluate_medical_terminology(response)
            
            # 3. Clinical reasoning quality
            metrics["clinical_reasoning"] = self._evaluate_clinical_reasoning(response, question)
            
            # 4. Knowledge integration
            metrics["knowledge_integration"] = self._evaluate_knowledge_integration(response, retrieved_docs)
            
            # 5. Comprehensiveness
            metrics["comprehensiveness"] = self._evaluate_comprehensiveness(response, question)
            
            # 6. Calculate overall score
            metrics["overall_score"] = self._calculate_mirage_score(metrics)
            
            logger.debug(f"   üìä MIRAGE evaluation complete. Overall score: {metrics['overall_score']:.3f}")
            
        except Exception as e:
            logger.error(f"‚ùå MIRAGE evaluation failed: {e}")
            # Return default scores on error
            metrics = {
                "answer_accuracy": 0.0,
                "medical_terminology": 0.0,
                "clinical_reasoning": 0.0,
                "knowledge_integration": 0.0,
                "comprehensiveness": 0.0,
                "overall_score": 0.0,
                "error": str(e)
            }
        
        return metrics
    
    def _evaluate_answer_accuracy(self, response: str, question: Dict) -> float:
        """Evaluate if the response contains the correct answer."""
        correct_answer = question.get("answer", "").strip()
        if not correct_answer:
            return 0.0
        
        # Normalize both response and answer
        response_clean = self._normalize_text(response)
        answer_clean = self._normalize_text(correct_answer)
        
        # Check for exact answer match
        if correct_answer.upper() in response.upper():
            return 1.0
        
        # Check for option letter match (A, B, C, D)
        if len(correct_answer) == 1 and correct_answer.isalpha():
            if correct_answer.upper() in response.upper():
                return 1.0
        
        # Semantic similarity check if available
        if hasattr(self, 'similarity_model') and self.similarity_model:
            try:
                embeddings = self.similarity_model.encode([response, correct_answer])
                similarity = np.dot(embeddings[0], embeddings[1]) / (
                    np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
                )
                return max(0.0, float(similarity))
            except:
                pass
        
        return 0.0
    
    def _evaluate_medical_terminology(self, response: str) -> float:
        """Evaluate usage of appropriate medical terminology."""
        if not response:
            return 0.0
        
        response_lower = response.lower()
        terminology_score = 0.0
        total_categories = len(self.medical_patterns)
        
        for category, terms in self.medical_patterns.items():
            category_matches = sum(1 for term in terms if term in response_lower)
            if category_matches > 0:
                terminology_score += min(1.0, category_matches / len(terms))
        
        return terminology_score / total_categories if total_categories > 0 else 0.0
    
    def _evaluate_clinical_reasoning(self, response: str, question: Dict) -> float:
        """Evaluate the quality of clinical reasoning in the response."""
        if not response:
            return 0.0
        
        response_lower = response.lower()
        reasoning_score = 0.0
        total_indicators = len(self.reasoning_indicators)
        
        for indicator_type, indicators in self.reasoning_indicators.items():
            indicator_matches = sum(1 for indicator in indicators if indicator in response_lower)
            if indicator_matches > 0:
                reasoning_score += min(1.0, indicator_matches / len(indicators))
        
        # Bonus for systematic approach
        if any(phrase in response_lower for phrase in ['step by step', 'first', 'second', 'then', 'therefore']):
            reasoning_score += 0.2
        
        return min(1.0, reasoning_score / total_indicators) if total_indicators > 0 else 0.0
    
    def _evaluate_knowledge_integration(self, response: str, retrieved_docs: List[Dict]) -> float:
        """Evaluate how well the response integrates retrieved knowledge."""
        if not response or not retrieved_docs:
            return 0.0
        
        response_lower = response.lower()
        integration_score = 0.0
        
        # Check for integration of retrieved content
        for doc in retrieved_docs[:5]:  # Check top 5 documents
            doc_content = doc.get('content', '') + ' ' + doc.get('title', '')
            if doc_content:
                doc_words = set(self._normalize_text(doc_content).split())
                response_words = set(self._normalize_text(response).split())
                
                if doc_words and response_words:
                    overlap = len(doc_words.intersection(response_words))
                    if overlap > 0:
                        integration_score += min(0.2, overlap / len(doc_words))
        
        return min(1.0, integration_score)
    
    def _evaluate_comprehensiveness(self, response: str, question: Dict) -> float:
        """Evaluate comprehensiveness of the response."""
        if not response:
            return 0.0
        
        # Length-based heuristic (with diminishing returns)
        response_length = len(response.split())
        length_score = min(1.0, response_length / 100)  # Optimal around 100 words
        
        # Coverage of question aspects
        question_text = question.get("question", "")
        question_words = set(self._normalize_text(question_text).split())
        response_words = set(self._normalize_text(response).split())
        
        coverage_score = 0.0
        if question_words:
            coverage_score = len(question_words.intersection(response_words)) / len(question_words)
        
        return (length_score + coverage_score) / 2
    
    def _calculate_mirage_score(self, metrics: Dict) -> float:
        """Calculate weighted overall MIRAGE score."""
        weights = {
            "answer_accuracy": 0.4,      # Most important
            "clinical_reasoning": 0.25,   # Very important
            "medical_terminology": 0.15,  # Important
            "knowledge_integration": 0.1, # Moderately important
            "comprehensiveness": 0.1      # Least weight
        }
        
        weighted_score = 0.0
        total_weight = 0.0
        
        for metric, weight in weights.items():
            if metric in metrics and metrics[metric] is not None:
                weighted_score += metrics[metric] * weight
                total_weight += weight
        
        return weighted_score / total_weight if total_weight > 0 else 0.0
    
    def _normalize_text(self, text: str) -> str:
        """Normalize text for comparison."""
        if not text:
            return ""
        
        import re
        # Remove extra whitespace and punctuation, convert to lowercase
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        text = re.sub(r'\s+', ' ', text.strip())
        return text
    
    def _init_evaluation_models(self) -> None:
        """Initialize models needed for evaluation."""
        try:
            # Try to initialize sentence transformer for semantic similarity
            from sentence_transformers import SentenceTransformer
            self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("‚úÖ Initialized similarity model for MIRAGE evaluation")
        except ImportError:
            logger.warning("‚ö†Ô∏è sentence-transformers not available, using basic evaluation")
            self.similarity_model = None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to initialize similarity model: {e}")
            self.similarity_model = None


# Fixed Data Loader for MIRAGE
# src/evaluation/data/data_loader.py

# Import official MIRAGE QADataset utility
try:
    from mirage.src.utils import QADataset
    MIRAGE_UTILS_AVAILABLE = True
    logger.info("‚úÖ Successfully imported QADataset from MIRAGE")
except ImportError as e:
    MIRAGE_UTILS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è QADataset from MIRAGE not available: {e} - using fallback implementation")


class BenchmarkDataLoader:
    """Load benchmark datasets for medical RAG evaluation."""
    
    def _load_mirage_with_qadataset(self, split: str = "test") -> List[Dict[str, Any]]:
        """Load MIRAGE data using official QADataset utility."""
        logger.info("üìö Loading MIRAGE using official QADataset...")
        
        try:
            # Try official QADataset first
            if MIRAGE_UTILS_AVAILABLE:
                logger.info("   üéØ Using official QADataset from MIRAGE")
                
                # Load all MIRAGE datasets
                mirage_datasets = ["mmlu", "medqa", "medmcqa", "pubmedqa", "bioasq"]
                all_data = []
                
                for dataset_name in mirage_datasets:
                    try:
                        # CORRECTED: Use only dataset_name parameter
                        qa_dataset = QADataset(dataset_name)
                        
                        # Get dataset size
                        dataset_size = len(qa_dataset)
                        logger.info(f"   üìä {dataset_name}: {dataset_size} questions")
                        
                        # Load all items from this dataset
                        for idx in range(dataset_size):
                            item = qa_dataset[idx]
                            processed_item = {
                                "question_id": f"{dataset_name}_{idx}",
                                "question": item.get("question", ""),
                                "context": "",
                                "answer": item.get("answer", ""),
                                "explanation": "",
                                "options": item.get("options", {}),
                                "question_type": "multiple_choice",
                                "medical_specialty": "general_medicine",
                                "reasoning_type": "clinical_reasoning",
                                "difficulty": "medium",
                                "benchmark": "mirage",
                                "source_dataset": dataset_name,
                                "source": "official_qadataset"
                            }
                            all_data.append(processed_item)
                    
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è Failed to load {dataset_name}: {e}")
                        continue
                
                if all_data:
                    logger.info(f"   ‚úÖ Loaded {len(all_data)} total MIRAGE samples")
                    return all_data
                else:
                    logger.warning("   ‚ö†Ô∏è No data loaded, falling back...")
            
            # Fallback to manual loading
            return self._load_official_mirage_data_fallback()
            
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è QADataset loading failed: {e}. Using fallback method.")
            return self._load_official_mirage_data_fallback()


# Setup instructions for MIRAGE submodule
# Add to your setup script:

"""
To fix the MIRAGE integration issue:

1. Clone the MIRAGE repository as a submodule:
   cd /path/to/your/project
   git submodule add https://github.com/Teddy-XiongGZ/MIRAGE.git mirage
   git submodule update --init --recursive

2. Or manually clone it:
   cd /path/to/your/project
   git clone https://github.com/Teddy-XiongGZ/MIRAGE.git mirage

3. Verify the structure:
   mirage/
   ‚îú‚îÄ‚îÄ src/
   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py  (contains QADataset class)
   ‚îú‚îÄ‚îÄ benchmark.json
   ‚îî‚îÄ‚îÄ README.md

4. The fixed code will automatically detect the mirage directory and use the correct QADataset import.

Key fixes made:
- Removed extra parameters (split, cache_dir) from QADataset initialization
- Added proper path detection for MIRAGE submodule
- Implemented fallback mechanisms
- Fixed import paths
- Added support for all MIRAGE datasets (mmlu, medqa, medmcqa, pubmedqa, bioasq)
"""