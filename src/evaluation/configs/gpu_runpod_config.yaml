# src/evaluation/configs/gpu_runpod_config.yaml
# Updated GPU RunPod Configuration - PubMedQA Disabled (Already in MIRAGE)

# General settings
enable_result_cache: true
max_memory_usage: 0.9  # Use 90% of available GPU memory
precision: "float16"   # Use mixed precision for GPU optimization

# Benchmark configurations with GPU optimization
benchmarks:
  mirage:
    enabled: true
    name: "MIRAGE"
    data_path: "data/benchmarks/mirage"
    cache_file: "mirage_benchmark.json"
    
    # GPU-optimized settings
    batch_size: 32  # Questions per batch
    max_samples: null  # Use full dataset
    quick_eval_samples: 100  # For quick testing
    
    # Evaluation parameters
    metrics: ["accuracy", "f1", "precision", "recall", "clinical_accuracy"]
    target_accuracy: 0.74
    
    # GPU-specific optimizations
    prefetch_batches: 2
    num_workers: 4
    pin_memory: true
    
    # MIRAGE-specific QADataset integration
    use_official_dataset: true
    dataset_source: "https://github.com/Teddy-XiongGZ/MIRAGE"
    
  medreason:
    enabled: true
    name: "MedReason"
    data_path: "data/benchmarks/medreason"
    cache_file: "medreason_benchmark.json"
    
    # GPU-optimized settings
    batch_size: 16  # Reasoning chains per batch
    max_samples: null
    quick_eval_samples: 50
    
    # Evaluation parameters
    metrics: ["accuracy", "reasoning_score", "consistency", "reasoning_chain_quality"]
    target_accuracy: 0.71
    
    # GPU-specific optimizations
    prefetch_batches: 2
    num_workers: 4
    pin_memory: true
    
  # PubMedQA DISABLED - Already included in MIRAGE benchmark
  pubmedqa:
    enabled: false  # DISABLED: Already covered by MIRAGE
    name: "PubMedQA"
    disabled_reason: "PubMedQA questions are already included in the MIRAGE benchmark"
    alternative: "Use MIRAGE benchmark which includes PubMedQA-style research questions"
    
  msmarco:
    enabled: true
    name: "MS MARCO"
    data_path: "data/benchmarks/msmarco"
    cache_file: "msmarco_benchmark.json"
    
    # GPU-optimized settings
    batch_size: 128  # Maximum batch for embeddings
    max_samples: null
    quick_eval_samples: 1000
    
    # Evaluation parameters
    metrics: ["mrr", "ndcg", "recall_at_k", "precision_at_k"]
    target_mrr: 0.35
    
    # GPU-specific optimizations
    prefetch_batches: 3
    num_workers: 6
    pin_memory: true

# System configurations
systems:
  kg_system:
    enabled: true
    name: "Knowledge Graph Enhanced RAG"
    model_path: "models/kg_enhanced"
    
  hierarchical_system:
    enabled: true
    name: "Hierarchical Diagnostic RAG"
    model_path: "models/hierarchical"

# GPU memory management
memory_management:
  clear_cache_between_benchmarks: true
  gradient_checkpointing: true
  mixed_precision: true
  max_sequence_length: 2048

# Output configuration
output:
  save_detailed_results: true
  generate_visualizations: true
  export_formats: ["json", "csv", "html"]
  
# Performance monitoring
monitoring:
  track_gpu_usage: true
  log_memory_usage: true
  benchmark_timing: true