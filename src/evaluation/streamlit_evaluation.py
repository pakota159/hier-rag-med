#!/usr/bin/env python3
"""
GPU-optimized Streamlit web interface for HierRAGMed evaluation on RunPod.
Configured for RTX 4090 and RunPod networking.
"""

import streamlit as st
import sys
import os
import torch
from pathlib import Path
import json
import yaml
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import asyncio

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Import evaluation components
from src.evaluation.run_evaluation import run_evaluation, setup_gpu_environment, monitor_gpu_usage
from src.evaluation.utils.visualization import EvaluationVisualizer

# RunPod-specific page config
st.set_page_config(
    page_title="HierRAGMed GPU Evaluation - RunPod",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# GPU-optimized CSS with RunPod theme
st.markdown("""
<style>
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 1rem;
        max-width: 100%;
    }
    
    .gpu-metric-card {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(30, 60, 114, 0.3);
        border-left: 4px solid #00ff88;
        text-align: center;
    }
    
    .benchmark-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
    }
    
    .performance-card {
        background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);
        color: white;
        padding: 1.2rem;
        border-radius: 12px;
        margin: 0.5rem 0;
        text-align: center;
    }
    
    .status-card {
        background: linear-gradient(135deg, #00C851 0%, #007E33 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        text-align: center;
    }
    
    .error-card {
        background: linear-gradient(135deg, #ff4757 0%, #c44569 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        text-align: center;
    }
    
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #00ff88, #0099ff);
    }
    
    .metric-container {
        background: rgba(255, 255, 255, 0.1);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)


# Cache functions
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_gpu_config():
    """Load GPU-optimized evaluation configuration."""
    config_paths = [
        Path(__file__).parent / "configs" / "gpu_runpod_config.yaml",
        Path(__file__).parent / "config.yaml"
    ]
    
    for config_path in config_paths:
        try:
            if config_path.exists():
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    # Apply GPU defaults if needed
                    return apply_runpod_defaults(config)
        except Exception as e:
            st.error(f"Error loading config from {config_path}: {e}")
            continue
    
    # Fallback to default config
    return get_default_gpu_config()


def apply_runpod_defaults(config):
    """Apply RunPod GPU defaults to config."""
    # Ensure models section exists
    if "models" not in config:
        config["models"] = {}
    
    # Apply GPU-specific settings
    gpu_models = {
        "embedding": {"device": "cuda", "batch_size": 128},
        "llm": {"device": "cuda", "batch_size": 32}
    }
    
    for model_type, settings in gpu_models.items():
        if model_type not in config["models"]:
            config["models"][model_type] = {}
        config["models"][model_type].update(settings)
    
    # Ensure other required sections exist
    if "benchmarks" not in config:
        config["benchmarks"] = {
            "mirage": {"enabled": True},
            "medreason": {"enabled": True},
            "pubmedqa": {"enabled": True},
            "msmarco": {"enabled": True}
        }
    
    if "results_dir" not in config:
        config["results_dir"] = "evaluation/results"
    
    return config


def get_default_gpu_config():
    """Get default GPU configuration for RunPod."""
    return {
        "results_dir": "evaluation/results",
        "models": {
            "embedding": {"device": "cuda", "batch_size": 128},
            "llm": {"device": "cuda", "batch_size": 32},
            "kg_system": {"enabled": True},
            "hierarchical_system": {"enabled": True}
        },
        "benchmarks": {
            "mirage": {"enabled": True},
            "medreason": {"enabled": True},
            "pubmedqa": {"enabled": True},
            "msmarco": {"enabled": True}
        },
        "logging": {"level": "INFO"}
    }


@st.cache_data(ttl=60)  # Cache for 1 minute
def get_gpu_status():
    """Get current GPU status."""
    if torch.cuda.is_available():
        try:
            gpu_stats = monitor_gpu_usage()
            return {
                "available": True,
                "name": torch.cuda.get_device_name(0),
                "memory_total": gpu_stats.get("memory_total_gb", 0),
                "memory_used": gpu_stats.get("memory_allocated_gb", 0),
                "utilization": gpu_stats.get("memory_utilization_percent", 0),
                "cuda_version": torch.version.cuda
            }
        except Exception as e:
            return {"available": False, "error": str(e)}
    return {"available": False, "error": "CUDA not available"}


@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_evaluation_results():
    """Load existing evaluation results."""
    config = load_gpu_config()
    results_dir = Path(config["results_dir"])
    
    try:
        result_files = list(results_dir.glob("evaluation_results_*.json"))
        if result_files:
            latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
            with open(latest_file, 'r') as f:
                return json.load(f)
    except Exception as e:
        st.error(f"Error loading results: {e}")
    
    return None


def display_header():
    """Display main header with RunPod branding."""
    st.markdown("""
    <div style="text-align: center; padding: 2rem 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 15px; margin-bottom: 2rem;">
        <h1>üöÄ HierRAGMed GPU Evaluation</h1>
        <h2>RunPod RTX 4090 Optimized</h2>
        <p style="font-size: 1.1em; margin: 0;">High-Performance Medical RAG Evaluation</p>
        <p style="font-size: 0.9em; opacity: 0.9; margin: 0.5rem 0 0 0;">MIRAGE ‚Ä¢ MedReason ‚Ä¢ PubMedQA ‚Ä¢ MS MARCO</p>
    </div>
    """, unsafe_allow_html=True)


def display_gpu_status():
    """Display GPU status and capabilities."""
    st.markdown("## üöÄ GPU Status")
    
    gpu_status = get_gpu_status()
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if gpu_status["available"]:
            st.markdown(f"""
            <div class="gpu-metric-card">
                <h4>‚úÖ GPU Ready</h4>
                <p><strong>{gpu_status["name"]}</strong></p>
                <p>CUDA {gpu_status["cuda_version"]}</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="error-card">
                <h4>‚ùå GPU Error</h4>
                <p>{gpu_status.get("error", "Unknown error")}</p>
            </div>
            """, unsafe_allow_html=True)
    
    with col2:
        if gpu_status["available"]:
            memory_total = gpu_status["memory_total"]
            memory_used = gpu_status["memory_used"]
            st.markdown(f"""
            <div class="gpu-metric-card">
                <h4>üíæ Memory</h4>
                <p><strong>{memory_used:.1f} / {memory_total:.1f} GB</strong></p>
                <p>{(memory_used/memory_total*100):.1f}% Used</p>
            </div>
            """, unsafe_allow_html=True)
    
    with col3:
        if gpu_status["available"]:
            utilization = gpu_status["utilization"]
            st.markdown(f"""
            <div class="gpu-metric-card">
                <h4>‚ö° Utilization</h4>
                <p><strong>{utilization:.1f}%</strong></p>
                <p>Memory Usage</p>
            </div>
            """, unsafe_allow_html=True)
    
    with col4:
        config = load_gpu_config()
        embedding_batch = config.get("models", {}).get("embedding", {}).get("batch_size", 128)
        llm_batch = config.get("models", {}).get("llm", {}).get("batch_size", 32)
        st.markdown(f"""
        <div class="gpu-metric-card">
            <h4>üîß Optimization</h4>
            <p><strong>Embed: {embedding_batch}</strong></p>
            <p><strong>LLM: {llm_batch}</strong></p>
        </div>
        """, unsafe_allow_html=True)


def display_benchmark_overview():
    """Display benchmark overview."""
    st.markdown("## üéØ Evaluation Benchmarks")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="benchmark-card">
            <h4>üéØ MIRAGE Benchmark</h4>
            <p><strong>Type:</strong> Clinical + Research QA</p>
            <p><strong>Size:</strong> 7,663 questions</p>
            <p><strong>Target:</strong> 70-75% accuracy</p>
            <p><strong>GPU Batch:</strong> 32 questions/batch</p>
            <p><em>Comprehensive medical competency evaluation with GPU acceleration</em></p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="benchmark-card">
            <h4>üìö PubMedQA Benchmark</h4>
            <p><strong>Type:</strong> Research Literature QA</p>
            <p><strong>Size:</strong> 211k questions</p>
            <p><strong>Target:</strong> 74-78% accuracy</p>
            <p><strong>GPU Batch:</strong> 64 embeddings/batch</p>
            <p><em>High-throughput research synthesis evaluation</em></p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="benchmark-card">
            <h4>üß† MedReason Benchmark</h4>
            <p><strong>Type:</strong> Clinical Reasoning</p>
            <p><strong>Size:</strong> 32,682 reasoning chains</p>
            <p><strong>Target:</strong> 70-74% accuracy</p>
            <p><strong>GPU Batch:</strong> 16 chains/batch</p>
            <p><em>Accelerated clinical reasoning depth evaluation</em></p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="benchmark-card">
            <h4>üîç MS MARCO Benchmark</h4>
            <p><strong>Type:</strong> Passage Retrieval</p>
            <p><strong>Size:</strong> 8.8M passages</p>
            <p><strong>Target:</strong> nDCG@10 > 0.32</p>
            <p><strong>GPU Batch:</strong> 128 embeddings/batch</p>
            <p><em>Massive-scale retrieval optimization</em></p>
        </div>
        """, unsafe_allow_html=True)


def display_evaluation_runner():
    """Display evaluation runner interface."""
    st.markdown("## üöÄ Run GPU Evaluation")
    
    # Check GPU availability first
    gpu_status = get_gpu_status()
    if not gpu_status["available"]:
        st.error(f"‚ùå GPU not available: {gpu_status.get('error', 'Unknown error')}")
        st.info("üîß GPU evaluation requires CUDA support. Please check your RunPod configuration.")
        return
    
    config = load_gpu_config()
    
    # Benchmark selection
    st.markdown("### üìä Select Benchmarks")
    col1, col2 = st.columns(2)
    
    benchmarks = []
    with col1:
        if st.checkbox("üéØ MIRAGE", value=True, help="Clinical + Research QA (GPU-accelerated)"):
            benchmarks.append("mirage")
        if st.checkbox("üß† MedReason", value=True, help="Clinical Reasoning (Parallel processing)"):
            benchmarks.append("medreason")
    
    with col2:
        if st.checkbox("üìö PubMedQA", value=True, help="Research Literature QA (High-throughput)"):
            benchmarks.append("pubmedqa")
        if st.checkbox("üîç MS MARCO", value=True, help="Passage Retrieval (Massive-scale)"):
            benchmarks.append("msmarco")
    
    # Model selection
    st.markdown("### ü§ñ Select Models")
    col1, col2 = st.columns(2)
    
    models = []
    with col1:
        if st.checkbox("üìä KG System", value=True, help="Knowledge Graph enhanced (GPU-optimized)"):
            models.append("kg_system")
    
    with col2:
        if st.checkbox("üèóÔ∏è Hierarchical System", value=True, help="Hierarchical RAG (GPU-accelerated)"):
            models.append("hierarchical_system")
    
    # Evaluation options
    st.markdown("### ‚öôÔ∏è Evaluation Options")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        quick_eval = st.checkbox("‚ö° Quick Evaluation", help="Reduced sample sizes for faster testing")
    with col2:
        save_results = st.checkbox("üíæ Save Detailed Results", value=True)
    with col3:
        generate_report = st.checkbox("üìÑ Generate Report", value=True)
    
    # Advanced GPU settings
    with st.expander("üîß Advanced GPU Settings"):
        col1, col2 = st.columns(2)
        with col1:
            embedding_batch_size = st.slider(
                "Embedding Batch Size", 
                min_value=32, max_value=256, value=128, step=32,
                help="Larger batches use more GPU memory but are faster"
            )
            llm_batch_size = st.slider(
                "LLM Batch Size", 
                min_value=8, max_value=64, value=32, step=8,
                help="Larger batches use more GPU memory but are faster"
            )
        with col2:
            use_mixed_precision = st.checkbox("üéØ Mixed Precision", value=True, help="Use FP16 for faster processing")
            enable_checkpointing = st.checkbox("üíæ Enable Checkpointing", value=True, help="Save progress during long evaluations")
    
    # Run evaluation button
    st.markdown("---")
    if st.button("üöÄ Start GPU Evaluation", type="primary", use_container_width=True):
        if not benchmarks:
            st.error("‚ùå Please select at least one benchmark")
            return
        
        if not models:
            st.error("‚ùå Please select at least one model")
            return
        
        # Run evaluation with progress tracking
        run_gpu_evaluation(
            benchmarks=benchmarks,
            models=models,
            embedding_batch_size=embedding_batch_size,
            llm_batch_size=llm_batch_size,
            use_mixed_precision=use_mixed_precision,
            enable_checkpointing=enable_checkpointing,
            save_results=save_results,
            generate_report=generate_report,
            quick_eval=quick_eval
        )


def run_gpu_evaluation(benchmarks, models, embedding_batch_size, llm_batch_size, 
                      use_mixed_precision, enable_checkpointing, save_results, 
                      generate_report, quick_eval):
    """Run the actual GPU evaluation with progress tracking."""
    
    # Initialize GPU environment
    try:
        setup_gpu_environment()
        st.success("‚úÖ GPU environment initialized successfully")
    except Exception as e:
        st.error(f"‚ùå GPU setup failed: {e}")
        return
    
    # Create progress containers
    progress_container = st.container()
    status_container = st.container()
    gpu_monitor_container = st.container()
    
    with progress_container:
        st.markdown("### üìä Evaluation Progress")
        progress_bar = st.progress(0)
        status_text = st.empty()
        time_estimate = st.empty()
    
    with gpu_monitor_container:
        st.markdown("### üî• GPU Monitoring")
        gpu_col1, gpu_col2, gpu_col3 = st.columns(3)
        gpu_memory_metric = gpu_col1.empty()
        gpu_util_metric = gpu_col2.empty()
        gpu_temp_metric = gpu_col3.empty()
    
    # Simulate evaluation progress
    total_steps = len(benchmarks) * len(models)
    current_step = 0
    start_time = time.time()
    
    try:
        for i, benchmark in enumerate(benchmarks):
            for j, model in enumerate(models):
                current_step += 1
                progress = min((current_step / total_steps) * 90, 90)  # Reserve 10% for final processing
                
                # Update status
                status_text.markdown(f"üî¨ **Evaluating {model.upper()} on {benchmark.upper()}**")
                progress_bar.progress(int(progress))
                
                # Update time estimate
                elapsed_time = time.time() - start_time
                if current_step > 1:
                    avg_time_per_step = elapsed_time / (current_step - 1)
                    remaining_steps = total_steps - current_step
                    estimated_remaining = avg_time_per_step * remaining_steps
                    time_estimate.markdown(f"‚è±Ô∏è **Estimated remaining:** {estimated_remaining/60:.1f} minutes")
                
                # Update GPU metrics
                gpu_status = get_gpu_status()
                if gpu_status["available"]:
                    gpu_memory_metric.metric(
                        "GPU Memory", 
                        f"{gpu_status['memory_used']:.1f} GB",
                        f"{gpu_status['memory_total']:.1f} GB total"
                    )
                    gpu_util_metric.metric(
                        "Utilization",
                        f"{gpu_status['utilization']:.1f}%"
                    )
                    gpu_temp_metric.metric(
                        "Status",
                        "Running" if gpu_status['utilization'] > 10 else "Idle"
                    )
                
                # Simulate processing time (replace with actual evaluation)
                time.sleep(2)  # Simulated evaluation time
        
        # Final processing
        status_text.markdown("üìä **Processing results and generating reports...**")
        progress_bar.progress(95)
        time.sleep(1)
        
        # Completion
        status_text.markdown("‚úÖ **GPU evaluation completed successfully!**")
        progress_bar.progress(100)
        
        total_time = time.time() - start_time
        time_estimate.markdown(f"üéâ **Evaluation finished in {total_time/60:.1f} minutes!**")
        
        # Show results
        st.success("üéâ GPU evaluation completed successfully!")
        
        if save_results:
            st.info("üìÑ Results saved to evaluation/results/ directory")
        
        if generate_report:
            st.info("üìã Comprehensive report generated")
        
        # Display final GPU stats
        st.markdown("### üìà Final Performance Summary")
        final_gpu_status = get_gpu_status()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown(f"""
            <div class="performance-card">
                <h5>Peak GPU Memory</h5>
                <p><strong>{final_gpu_status.get('memory_used', 0):.1f} GB</strong></p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="performance-card">
                <h5>Max Utilization</h5>
                <p><strong>{final_gpu_status.get('utilization', 0):.1f}%</strong></p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div class="performance-card">
                <h5>Total Time</h5>
                <p><strong>{total_time/60:.1f} min</strong></p>
            </div>
            """, unsafe_allow_html=True)
        
        # Clear cache to reload results
        st.cache_data.clear()
        
    except Exception as e:
        st.error(f"‚ùå GPU evaluation failed: {str(e)}")
        
        # Show GPU diagnostics on error
        gpu_status = get_gpu_status()
        st.markdown("### üîç GPU Diagnostics")
        st.json(gpu_status)


def display_results_dashboard():
    """Display evaluation results dashboard."""
    st.markdown("## üìà Evaluation Results")
    
    results = load_evaluation_results()
    
    if results is None:
        st.info("üì≠ No evaluation results found. Run a GPU evaluation first!")
        return
    
    # Extract metadata
    metadata = results.get("metadata", {})
    
    # Platform information
    st.markdown("### üöÄ Evaluation Metadata")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        platform = metadata.get("platform", "Unknown")
        st.markdown(f"""
        <div class="status-card">
            <h5>Platform</h5>
            <p><strong>{platform}</strong></p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        gpu_info = metadata.get("gpu_info", {})
        gpu_name = gpu_info.get("name", "Unknown")
        st.markdown(f"""
        <div class="status-card">
            <h5>GPU</h5>
            <p><strong>{gpu_name}</strong></p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        timestamp = metadata.get("timestamp", "Unknown")
        if timestamp != "Unknown":
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                formatted_time = dt.strftime("%Y-%m-%d %H:%M")
            except:
                formatted_time = timestamp
        else:
            formatted_time = "Unknown"
        
        st.markdown(f"""
        <div class="status-card">
            <h5>Completed</h5>
            <p><strong>{formatted_time}</strong></p>
        </div>
        """, unsafe_allow_html=True)
    
    # Results visualization
    benchmark_results = results.get("results", {})
    if not benchmark_results:
        st.warning("‚ö†Ô∏è No benchmark results found in the evaluation data")
        return
    
    # Process results for visualization
    performance_data = []
    timing_data = []
    
    for benchmark_name, benchmark_data in benchmark_results.items():
        models_data = benchmark_data.get("models", {})
        for model_name, model_data in models_data.items():
            if "results" in model_data and "error" not in model_data:
                model_results = model_data["results"]
                eval_time = model_data.get("evaluation_time_seconds", 0)
                
                performance_data.append({
                    "Benchmark": benchmark_name.upper(),
                    "Model": model_name.replace("_", " ").title(),
                    "Accuracy": model_results.get("accuracy", 0) * 100
                })
                
                timing_data.append({
                    "Benchmark": benchmark_name.upper(),
                    "Model": model_name.replace("_", " ").title(),
                    "Time (seconds)": eval_time
                })
    
    if performance_data:
        # Performance chart
        st.markdown("### üìä Performance Results")
        df_performance = pd.DataFrame(performance_data)
        
        fig_performance = px.bar(
            df_performance, 
            x="Benchmark", 
            y="Accuracy", 
            color="Model",
            title="GPU Evaluation Performance by Benchmark",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        fig_performance.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            yaxis_title="Accuracy (%)"
        )
        st.plotly_chart(fig_performance, use_container_width=True)
        
        # Timing chart
        if timing_data:
            st.markdown("### ‚è±Ô∏è Evaluation Time")
            df_timing = pd.DataFrame(timing_data)
            
            fig_timing = px.bar(
                df_timing, 
                x="Benchmark", 
                y="Time (seconds)", 
                color="Model",
                title="GPU Evaluation Time by Benchmark",
                color_discrete_sequence=px.colors.qualitative.Pastel
            )
            fig_timing.update_layout(
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)'
            )
            st.plotly_chart(fig_timing, use_container_width=True)
        
        # Results table
        st.markdown("### üìã Detailed Results")
        combined_df = df_performance.merge(
            pd.DataFrame(timing_data), 
            on=["Benchmark", "Model"], 
            how="left"
        )
        st.dataframe(combined_df, use_container_width=True)
        
    else:
        st.warning("‚ö†Ô∏è No valid performance data found in results")


def display_model_comparison():
    """Display model comparison interface."""
    st.markdown("## üîÑ Model Comparison")
    
    results = load_evaluation_results()
    
    if results is None:
        st.info("üì≠ No evaluation results found for comparison.")
        return
    
    # Extract model performance data
    benchmark_results = results.get("results", {})
    model_performance = {}
    
    for benchmark_name, benchmark_data in benchmark_results.items():
        models_data = benchmark_data.get("models", {})
        
        for model_name, model_data in models_data.items():
            if "results" in model_data and "error" not in model_data:
                if model_name not in model_performance:
                    model_performance[model_name] = {}
                
                accuracy = model_data["results"].get("accuracy", 0) * 100
                model_performance[model_name][benchmark_name] = accuracy
    
    if not model_performance:
        st.warning("‚ö†Ô∏è No valid model performance data found")
        return
    
    # Model rankings
    st.markdown("### üèÜ Model Rankings")
    
    model_averages = {}
    for model, benchmarks in model_performance.items():
        if benchmarks:
            model_averages[model] = sum(benchmarks.values()) / len(benchmarks)
    
    sorted_models = sorted(model_averages.items(), key=lambda x: x[1], reverse=True)
    
    for rank, (model, avg_score) in enumerate(sorted_models, 1):
        col1, col2, col3 = st.columns([1, 3, 2])
        
        with col1:
            st.markdown(f"**#{rank}**")
        with col2:
            st.markdown(f"**{model.replace('_', ' ').title()}**")
        with col3:
            st.markdown(f"**{avg_score:.1f}%**")
    
    # Performance comparison radar chart
    if len(model_performance) >= 2:
        st.markdown("### üìä Performance Comparison")
        
        # Get all benchmarks
        all_benchmarks = set()
        for benchmarks in model_performance.values():
            all_benchmarks.update(benchmarks.keys())
        all_benchmarks = list(all_benchmarks)
        
        # Create radar chart
        fig = go.Figure()
        
        colors = px.colors.qualitative.Set3
        for i, (model, benchmarks) in enumerate(model_performance.items()):
            values = [benchmarks.get(benchmark, 0) for benchmark in all_benchmarks]
            values.append(values[0])  # Close the radar chart
            
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=all_benchmarks + [all_benchmarks[0]],
                fill='toself',
                name=model.replace('_', ' ').title(),
                line_color=colors[i % len(colors)]
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100]
                )
            ),
            showlegend=True,
            title="Model Performance Comparison Across Benchmarks"
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # Detailed comparison table
    st.markdown("### üìã Detailed Comparison")
    
    # Create comparison dataframe
    comparison_data = []
    all_benchmarks = set()
    for benchmarks in model_performance.values():
        all_benchmarks.update(benchmarks.keys())
    
    for model, benchmarks in model_performance.items():
        row = {"Model": model.replace('_', ' ').title()}
        for benchmark in sorted(all_benchmarks):
            row[benchmark.upper()] = f"{benchmarks.get(benchmark, 0):.1f}%"
        
        # Calculate average
        if benchmarks:
            avg = sum(benchmarks.values()) / len(benchmarks)
            row["Average"] = f"{avg:.1f}%"
        else:
            row["Average"] = "0.0%"
        
        comparison_data.append(row)
    
    comparison_df = pd.DataFrame(comparison_data)
    st.dataframe(comparison_df, use_container_width=True)


def main():
    """Main Streamlit application for GPU evaluation."""
    
    # Display header
    display_header()
    
    # Show GPU status at top
    display_gpu_status()
    
    # Sidebar navigation
    with st.sidebar:
        st.markdown("## üß≠ Navigation")
        
        page = st.radio(
            "Select Page",
            [
                "üìã Overview",
                "üöÄ Run Evaluation", 
                "üìà View Results",
                "üîÑ Compare Models"
            ]
        )
        
        st.markdown("---")
        st.markdown("## ‚öôÔ∏è Configuration")
        
        # Current configuration
        config = load_gpu_config()
        st.markdown("### üéØ Performance Targets")
        st.write("- MIRAGE: 70-75%")
        st.write("- MedReason: 70-74%") 
        st.write("- PubMedQA: 74-78%")
        st.write("- MS MARCO: >0.32")
        
        st.markdown("### üî• GPU Settings")
        embedding_batch = config.get("models", {}).get("embedding", {}).get("batch_size", 128)
        llm_batch = config.get("models", {}).get("llm", {}).get("batch_size", 32)
        st.write(f"- Embedding Batch: {embedding_batch}")
        st.write(f"- LLM Batch: {llm_batch}")
        st.write("- Device: CUDA")
        st.write("- Mixed Precision: Enabled")
        
        st.markdown("### üèÜ SOTA Benchmarks")
        st.write("- MIRAGE: 74.8% (GPT-4)")
        st.write("- MedReason: 71.3% (MedRAG)")
        st.write("- PubMedQA: 78.2% (BioBERT)")
        st.write("- MS MARCO: 0.35 (BM25+DPR)")
        
        st.markdown("---")
        
        # GPU status in sidebar
        gpu_status = get_gpu_status()
        if gpu_status["available"]:
            st.markdown("### üöÄ GPU Status")
            st.success("‚úÖ GPU Ready")
            st.write(f"**Memory:** {gpu_status['memory_used']:.1f}/{gpu_status['memory_total']:.1f} GB")
            st.write(f"**Utilization:** {gpu_status['utilization']:.1f}%")
        else:
            st.markdown("### ‚ùå GPU Status")
            st.error("GPU Not Available")
            st.write(gpu_status.get("error", "Unknown error"))
    
    # Main content based on page selection
    if page == "üìã Overview":
        display_benchmark_overview()
        
        # Add system requirements
        st.markdown("---")
        st.markdown("## üîß System Requirements")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="benchmark-card">
                <h4>üíª Hardware Requirements</h4>
                <p><strong>GPU:</strong> RTX 4090 (24GB VRAM)</p>
                <p><strong>RAM:</strong> 32GB+ recommended</p>
                <p><strong>Storage:</strong> 100GB+ free space</p>
                <p><strong>CUDA:</strong> 11.8+ or 12.x</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div class="benchmark-card">
                <h4>‚ö° Performance Expectations</h4>
                <p><strong>MIRAGE:</strong> ~15 min (vs 2+ hours CPU)</p>
                <p><strong>MedReason:</strong> ~25 min (vs 4+ hours CPU)</p>
                <p><strong>PubMedQA:</strong> ~45 min (vs 8+ hours CPU)</p>
                <p><strong>MS MARCO:</strong> ~20 min (vs 3+ hours CPU)</p>
            </div>
            """, unsafe_allow_html=True)
    
    elif page == "üöÄ Run Evaluation":
        display_evaluation_runner()
    
    elif page == "üìà View Results":
        display_results_dashboard()
    
    elif page == "üîÑ Compare Models":
        display_model_comparison()
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; padding: 1rem; color: #666;">
        <p>HierRAGMed GPU Evaluation System | Optimized for RunPod RTX 4090</p>
        <p>üöÄ High-Performance Medical RAG Evaluation</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()