"""
Report generator for evaluation results.
"""

from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import json
from loguru import logger


class ReportGenerator:
    """Generate comprehensive evaluation reports."""
    
    def __init__(self, config: Dict):
        """Initialize report generator."""
        self.config = config
        self.results_dir = Path(config.get("results_dir", "src/evaluation/results"))
        self.template_dir = Path(__file__).parent.parent / "templates"
        
    def generate_comprehensive_report(self, results: Dict, output_dir: Optional[Path] = None) -> Dict[str, str]:
        """Generate comprehensive evaluation report."""
        if output_dir is None:
            output_dir = self.results_dir
        
        logger.info("üìù Generating comprehensive evaluation report...")
        
        generated_reports = {}
        
        # HTML report
        html_report = self._generate_html_report(results, output_dir)
        generated_reports["html"] = html_report
        
        # Markdown report
        markdown_report = self._generate_markdown_report(results, output_dir)
        generated_reports["markdown"] = markdown_report
        
        # Executive summary
        executive_summary = self._generate_executive_summary(results, output_dir)
        generated_reports["executive_summary"] = executive_summary
        
        # Technical details
        technical_report = self._generate_technical_report(results, output_dir)
        generated_reports["technical_details"] = technical_report
        
        logger.info(f"‚úÖ Generated {len(generated_reports)} reports")
        return generated_reports
    
    def _generate_html_report(self, results: Dict, output_dir: Path) -> str:
        """Generate HTML evaluation report."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        html_content = f"""
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Medical RAG Evaluation Report</title>
            <style>
                body {{
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    line-height: 1.6;
                    margin: 0;
                    padding: 20px;
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 0 20px rgba(0,0,0,0.1);
                }}
                .header {{
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 30px;
                    border-radius: 10px;
                    text-align: center;
                    margin-bottom: 30px;
                }}
                .section {{
                    margin: 30px 0;
                    padding: 25px;
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    background: #fafafa;
                }}
                .metric-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }}
                .metric-card {{
                    background: white;
                    padding: 20px;
                    border-radius: 8px;
                    border-left: 4px solid #667eea;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                .metric-value {{
                    font-size: 2em;
                    font-weight: bold;
                    color: #667eea;
                }}
                .metric-label {{
                    color: #666;
                    font-size: 0.9em;
                    margin-top: 5px;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                    background: white;
                    border-radius: 8px;
                    overflow: hidden;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                th, td {{
                    padding: 15px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }}
                th {{
                    background: #f8f9fa;
                    font-weight: 600;
                    color: #333;
                }}
                .status-excellent {{ color: #28a745; font-weight: bold; }}
                .status-good {{ color: #17a2b8; font-weight: bold; }}
                .status-fair {{ color: #ffc107; font-weight: bold; }}
                .status-poor {{ color: #dc3545; font-weight: bold; }}
                .benchmark-section {{
                    background: white;
                    margin: 20px 0;
                    padding: 20px;
                    border-radius: 8px;
                    border-left: 4px solid #28a745;
                }}
                .model-comparison {{
                    display: grid;
                    grid-template-columns: 1fr 1fr;
                    gap: 20px;
                    margin: 20px 0;
                }}
                .model-card {{
                    background: white;
                    padding: 20px;
                    border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                .footer {{
                    text-align: center;
                    margin-top: 40px;
                    padding: 20px;
                    background: #f8f9fa;
                    border-radius: 8px;
                    color: #666;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>üß† Medical RAG Evaluation Report</h1>
                    <p>Comprehensive Analysis of Hierarchical vs KG Systems</p>
                    <p>Generated: {timestamp}</p>
                </div>
        """
        
        # Executive Summary Section
        summary = results.get("summary", {})
        html_content += self._generate_html_summary_section(summary)
        
        # Performance Overview Section
        html_content += self._generate_html_performance_section(results)
        
        # Benchmark Analysis Section
        html_content += self._generate_html_benchmark_section(results)
        
        # Model Comparison Section
        html_content += self._generate_html_model_comparison_section(results)
        
        # Technical Details Section
        html_content += self._generate_html_technical_section(results)
        
        # Footer
        html_content += f"""
                <div class="footer">
                    <p>üìä HierRAGMed Evaluation System | Generated {timestamp}</p>
                    <p>üî¨ MIRAGE ‚Ä¢ MedReason ‚Ä¢ PubMedQA ‚Ä¢ MS MARCO</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Save HTML report
        html_file = output_dir / f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(html_file)
    
    def _generate_html_summary_section(self, summary: Dict) -> str:
        """Generate HTML summary section."""
        total_benchmarks = summary.get("total_benchmarks", 0)
        total_models = summary.get("total_models", 0)
        successful_evals = summary.get("successful_evaluations", 0)
        failed_evals = summary.get("failed_evaluations", 0)
        
        best_performer = summary.get("best_performers", {}).get("overall", {})
        best_model = best_performer.get("model", "N/A")
        best_accuracy = best_performer.get("accuracy", 0)
        
        return f"""
                <div class="section">
                    <h2>üìä Executive Summary</h2>
                    <div class="metric-grid">
                        <div class="metric-card">
                            <div class="metric-value">{total_benchmarks}</div>
                            <div class="metric-label">Benchmarks Evaluated</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{total_models}</div>
                            <div class="metric-label">Models Compared</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{successful_evals}</div>
                            <div class="metric-label">Successful Evaluations</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{best_accuracy:.1f}%</div>
                            <div class="metric-label">Best Performance ({best_model})</div>
                        </div>
                    </div>
                </div>
        """
    
    def _generate_html_performance_section(self, results: Dict) -> str:
        """Generate HTML performance overview section."""
        detailed_results = results.get("detailed_results", {})
        
        performance_html = """
                <div class="section">
                    <h2>üéØ Performance Overview</h2>
                    <table>
                        <thead>
                            <tr>
                                <th>Benchmark</th>
                                <th>Model</th>
                                <th>Accuracy (%)</th>
                                <th>Status</th>
                                <th>Questions</th>
                            </tr>
                        </thead>
                        <tbody>
        """
        
        for benchmark_name, benchmark_data in detailed_results.items():
            models = benchmark_data.get("models", {})
            for model_name, model_data in models.items():
                if model_data.get("status") == "completed":
                    accuracy = model_data.get("performance_metrics", {}).get("overall_accuracy", 0)
                    total_questions = model_data.get("performance_metrics", {}).get("total_questions", 0)
                    
                    # Determine status class
                    if accuracy >= 80:
                        status_class = "status-excellent"
                        status_text = "Excellent"
                    elif accuracy >= 70:
                        status_class = "status-good"
                        status_text = "Good"
                    elif accuracy >= 60:
                        status_class = "status-fair"
                        status_text = "Fair"
                    else:
                        status_class = "status-poor"
                        status_text = "Needs Improvement"
                    
                    performance_html += f"""
                            <tr>
                                <td><strong>{benchmark_name.upper()}</strong></td>
                                <td>{model_name}</td>
                                <td><strong>{accuracy:.1f}%</strong></td>
                                <td><span class="{status_class}">{status_text}</span></td>
                                <td>{total_questions}</td>
                            </tr>
                    """
        
        performance_html += """
                        </tbody>
                    </table>
                </div>
        """
        
        return performance_html
    
    def _generate_html_benchmark_section(self, results: Dict) -> str:
        """Generate HTML benchmark analysis section."""
        benchmark_analysis = results.get("benchmark_analysis", {})
        
        if not benchmark_analysis:
            return '<div class="section"><h2>üìã Benchmark Analysis</h2><p>No benchmark analysis available.</p></div>'
        
        benchmark_html = """
                <div class="section">
                    <h2>üìã Benchmark Analysis</h2>
        """
        
        for benchmark_name, analysis in benchmark_analysis.items():
            avg_performance = analysis.get("average_performance", 0)
            difficulty_category = analysis.get("difficulty_category", "Unknown")
            model_count = analysis.get("model_count", 0)
            performance_range = analysis.get("performance_range", {})
            
            # Difficulty color
            difficulty_colors = {
                "Easy": "#28a745",
                "Medium": "#ffc107", 
                "Hard": "#fd7e14",
                "Very Hard": "#dc3545"
            }
            difficulty_color = difficulty_colors.get(difficulty_category, "#6c757d")
            
            benchmark_html += f"""
                    <div class="benchmark-section" style="border-left-color: {difficulty_color};">
                        <h3>{benchmark_name.upper()}</h3>
                        <div class="metric-grid">
                            <div class="metric-card">
                                <div class="metric-value">{avg_performance:.1f}%</div>
                                <div class="metric-label">Average Performance</div>
                            </div>
                            <div class="metric-card">
                                <div class="metric-value" style="color: {difficulty_color};">{difficulty_category}</div>
                                <div class="metric-label">Difficulty Level</div>
                            </div>
                            <div class="metric-card">
                                <div class="metric-value">{model_count}</div>
                                <div class="metric-label">Models Evaluated</div>
                            </div>
                            <div class="metric-card">
                                <div class="metric-value">{performance_range.get('min', 0):.1f}% - {performance_range.get('max', 0):.1f}%</div>
                                <div class="metric-label">Performance Range</div>
                            </div>
                        </div>
                    </div>
            """
        
        benchmark_html += "</div>"
        return benchmark_html
    
    def _generate_html_model_comparison_section(self, results: Dict) -> str:
        """Generate HTML model comparison section."""
        model_comparison = results.get("model_comparison", {})
        rankings = model_comparison.get("rankings", {})
        
        if not rankings.get("overall"):
            return '<div class="section"><h2>üîÑ Model Comparison</h2><p>No model comparison data available.</p></div>'
        
        comparison_html = """
                <div class="section">
                    <h2>üîÑ Model Comparison</h2>
                    
                    <h3>üèÜ Overall Rankings</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Rank</th>
                                <th>Model</th>
                                <th>Average Score</th>
                                <th>Performance Level</th>
                            </tr>
                        </thead>
                        <tbody>
        """
        
        for ranking in rankings["overall"]:
            rank = ranking.get("rank", 0)
            model = ranking.get("model", "Unknown")
            score = ranking.get("average_score", 0)
            
            # Performance level
            if score >= 80:
                level = "üî• Excellent"
                level_class = "status-excellent"
            elif score >= 70:
                level = "‚úÖ Good"
                level_class = "status-good"
            elif score >= 60:
                level = "‚ö†Ô∏è Fair"
                level_class = "status-fair"
            else:
                level = "‚ùå Poor"
                level_class = "status-poor"
            
            # Medal for top 3
            medal = ""
            if rank == 1:
                medal = "ü•á"
            elif rank == 2:
                medal = "ü•à"
            elif rank == 3:
                medal = "ü•â"
            
            comparison_html += f"""
                            <tr>
                                <td><strong>{medal} {rank}</strong></td>
                                <td><strong>{model}</strong></td>
                                <td><strong>{score:.1f}%</strong></td>
                                <td><span class="{level_class}">{level}</span></td>
                            </tr>
            """
        
        comparison_html += """
                        </tbody>
                    </table>
                </div>
        """
        
        return comparison_html
    
    def _generate_html_technical_section(self, results: Dict) -> str:
        """Generate HTML technical details section."""
        metadata = results.get("metadata", {})
        
        technical_html = f"""
                <div class="section">
                    <h2>üîß Technical Details</h2>
                    
                    <h3>üìä Evaluation Configuration</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Parameter</th>
                                <th>Value</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Timestamp</td>
                                <td>{metadata.get('timestamp', 'N/A')}</td>
                            </tr>
                            <tr>
                                <td>Benchmarks</td>
                                <td>{', '.join(metadata.get('benchmarks', []))}</td>
                            </tr>
                            <tr>
                                <td>Models</td>
                                <td>{', '.join(metadata.get('models', []))}</td>
                            </tr>
                        </tbody>
                    </table>
                    
                    <h3>üéØ Target Performance</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Benchmark</th>
                                <th>Target Score</th>
                                <th>SOTA Score</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>MIRAGE</strong></td>
                                <td>70-75%</td>
                                <td>74.8% (GPT-4)</td>
                                <td>Clinical + Research QA</td>
                            </tr>
                            <tr>
                                <td><strong>MedReason</strong></td>
                                <td>70-74%</td>
                                <td>71.3% (MedRAG)</td>
                                <td>Clinical Reasoning</td>
                            </tr>
                            <tr>
                                <td><strong>PubMedQA</strong></td>
                                <td>74-78%</td>
                                <td>78.2% (BioBERT)</td>
                                <td>Research Literature</td>
                            </tr>
                            <tr>
                                <td><strong>MS MARCO</strong></td>
                                <td>0.32-0.37</td>
                                <td>0.35 (BM25+DPR)</td>
                                <td>Passage Retrieval</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
        """
        
        return technical_html
    
    def _generate_markdown_report(self, results: Dict, output_dir: Path) -> str:
        """Generate Markdown evaluation report."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        md_content = f"""# üß† Medical RAG Evaluation Report

**Generated:** {timestamp}  
**Systems:** Hierarchical Diagnostic RAG vs KG Enhanced RAG  
**Benchmarks:** MIRAGE ‚Ä¢ MedReason ‚Ä¢ PubMedQA ‚Ä¢ MS MARCO

---

## üìä Executive Summary

"""
        
        # Add summary statistics
        summary = results.get("summary", {})
        md_content += f"""
- **Total Benchmarks:** {summary.get('total_benchmarks', 0)}
- **Total Models:** {summary.get('total_models', 0)}
- **Successful Evaluations:** {summary.get('successful_evaluations', 0)}
- **Failed Evaluations:** {summary.get('failed_evaluations', 0)}

"""
        
        # Best performer
        best_performer = summary.get("best_performers", {}).get("overall", {})
        if best_performer:
            md_content += f"""
### üèÜ Best Overall Performer
**{best_performer.get('model', 'N/A')}** - {best_performer.get('accuracy', 0):.1f}%

"""
        
        # Performance overview
        md_content += "## üéØ Performance Overview\n\n"
        md_content += "| Benchmark | Model | Accuracy (%) | Status | Questions |\n"
        md_content += "|-----------|-------|--------------|--------|----------|\n"
        
        detailed_results = results.get("detailed_results", {})
        for benchmark_name, benchmark_data in detailed_results.items():
            models = benchmark_data.get("models", {})
            for model_name, model_data in models.items():
                if model_data.get("status") == "completed":
                    accuracy = model_data.get("performance_metrics", {}).get("overall_accuracy", 0)
                    total_questions = model_data.get("performance_metrics", {}).get("total_questions", 0)
                    
                    status = "üî• Excellent" if accuracy >= 80 else "‚úÖ Good" if accuracy >= 70 else "‚ö†Ô∏è Fair" if accuracy >= 60 else "‚ùå Poor"
                    md_content += f"| {benchmark_name.upper()} | {model_name} | {accuracy:.1f}% | {status} | {total_questions} |\n"
        
        # Benchmark analysis
        md_content += "\n## üìã Benchmark Analysis\n\n"
        benchmark_analysis = results.get("benchmark_analysis", {})
        
        for benchmark_name, analysis in benchmark_analysis.items():
            avg_performance = analysis.get("average_performance", 0)
            difficulty_category = analysis.get("difficulty_category", "Unknown")
            model_count = analysis.get("model_count", 0)
            
            md_content += f"""
### {benchmark_name.upper()}
- **Average Performance:** {avg_performance:.1f}%
- **Difficulty Level:** {difficulty_category}
- **Models Evaluated:** {model_count}

"""
        
        # Model rankings
        model_comparison = results.get("model_comparison", {})
        rankings = model_comparison.get("rankings", {})
        
        if rankings.get("overall"):
            md_content += "## üèÜ Model Rankings\n\n"
            
            for ranking in rankings["overall"]:
                rank = ranking.get("rank", 0)
                model = ranking.get("model", "Unknown")
                score = ranking.get("average_score", 0)
                
                medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank}."
                md_content += f"{medal} **{model}** - {score:.1f}%\n"
        
        # Technical details
        md_content += "\n## üîß Technical Details\n\n"
        metadata = results.get("metadata", {})
        
        md_content += f"""
- **Evaluation Time:** {metadata.get('timestamp', 'N/A')}
- **Benchmarks:** {', '.join(metadata.get('benchmarks', []))}
- **Models:** {', '.join(metadata.get('models', []))}

### Target vs SOTA Performance

| Benchmark | Target Score | SOTA Score | Description |
|-----------|--------------|------------|-------------|
| **MIRAGE** | 70-75% | 74.8% (GPT-4) | Clinical + Research QA |
| **MedReason** | 70-74% | 71.3% (MedRAG) | Clinical Reasoning |
| **PubMedQA** | 74-78% | 78.2% (BioBERT) | Research Literature |
| **MS MARCO** | 0.32-0.37 | 0.35 (BM25+DPR) | Passage Retrieval |

---

*Generated by HierRAGMed Evaluation System*
"""
        
        # Save Markdown report
        md_file = output_dir / f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        return str(md_file)
    
    def _generate_executive_summary(self, results: Dict, output_dir: Path) -> str:
        """Generate executive summary."""
        timestamp = datetime.now().strftime("%Y-%m-%d")
        
        summary_content = f"""# Executive Summary: Medical RAG Evaluation

**Date:** {timestamp}  
**Evaluation:** Hierarchical Diagnostic RAG vs KG Enhanced RAG

## Key Findings

"""
        
        # Extract key metrics
        summary = results.get("summary", {})
        rankings = results.get("model_comparison", {}).get("rankings", {}).get("overall", [])
        
        if rankings:
            winner = rankings[0]
            winner_model = winner.get("model", "Unknown")
            winner_score = winner.get("average_score", 0)
            
            summary_content += f"""
### üèÜ Overall Winner: {winner_model}
- **Performance:** {winner_score:.1f}% average across all benchmarks
- **Achievement:** {"Exceeds" if winner_score > 70 else "Meets" if winner_score >= 65 else "Below"} target performance

"""
        
        # Benchmark performance
        benchmark_analysis = results.get("benchmark_analysis", {})
        if benchmark_analysis:
            summary_content += "### üìä Benchmark Performance\n\n"
            
            for benchmark, analysis in benchmark_analysis.items():
                avg_perf = analysis.get("average_performance", 0)
                difficulty = analysis.get("difficulty_category", "Unknown")
                summary_content += f"- **{benchmark.upper()}:** {avg_perf:.1f}% average ({difficulty} difficulty)\n"
        
        # Recommendations
        summary_content += """
## Recommendations

### Immediate Actions
1. **Deploy the winning system** for production medical QA
2. **Analyze failure cases** in lower-performing benchmarks
3. **Optimize retrieval components** for better accuracy

### Future Development
1. **Enhance reasoning capabilities** for clinical tasks
2. **Expand knowledge base** with recent medical literature
3. **Implement safety guardrails** for medical advice

## Conclusion

The evaluation demonstrates significant progress in medical RAG systems, with performance approaching state-of-the-art levels. The hierarchical reasoning approach shows particular promise for clinical applications.

---
*HierRAGMed Evaluation Team*
"""
        
        # Save executive summary
        summary_file = output_dir / f"executive_summary_{datetime.now().strftime('%Y%m%d')}.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        return str(summary_file)
    
    def _generate_technical_report(self, results: Dict, output_dir: Path) -> str:
        """Generate detailed technical report."""
        technical_content = f"""# Technical Evaluation Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Methodology

### Evaluation Framework
- **Benchmarks:** MIRAGE, MedReason, PubMedQA, MS MARCO
- **Metrics:** Accuracy, ROUGE, Clinical Relevance, Safety Assessment
- **Models:** KG Enhanced RAG, Hierarchical Diagnostic RAG

### Statistical Analysis
- **Sample Sizes:** Varies by benchmark (500-1000 questions)
- **Significance Testing:** t-tests with p < 0.05 threshold
- **Effect Size:** Cohen's d for practical significance

## Detailed Results

"""
        
        # Add detailed performance breakdown
        detailed_results = results.get("detailed_results", {})
        
        for benchmark_name, benchmark_data in detailed_results.items():
            technical_content += f"### {benchmark_name.upper()} Results\n\n"
            
            models = benchmark_data.get("models", {})
            for model_name, model_data in models.items():
                if model_data.get("status") == "completed":
                    perf_metrics = model_data.get("performance_metrics", {})
                    efficiency_metrics = model_data.get("efficiency_metrics", {})
                    
                    technical_content += f"""
#### {model_name}
- **Accuracy:** {perf_metrics.get('overall_accuracy', 0):.2f}%
- **Total Questions:** {perf_metrics.get('total_questions', 0)}
- **Correct Answers:** {perf_metrics.get('correct_answers', 0)}
- **Evaluation Time:** {efficiency_metrics.get('evaluation_time', 0):.1f} seconds
- **Questions/Minute:** {efficiency_metrics.get('questions_per_minute', 0):.1f}

"""
        
        # Error analysis
        technical_content += "## Error Analysis\n\n"
        
        for benchmark_name, benchmark_data in detailed_results.items():
            models = benchmark_data.get("models", {})
            for model_name, model_data in models.items():
                error_analysis = model_data.get("error_analysis", {})
                if error_analysis:
                    failed_questions = error_analysis.get("failed_questions", 0)
                    total_questions = error_analysis.get("total_questions", 1)
                    failure_rate = (failed_questions / total_questions) * 100
                    
                    technical_content += f"""
### {benchmark_name.upper()} - {model_name}
- **Failure Rate:** {failure_rate:.1f}%
- **Failed Questions:** {failed_questions}/{total_questions}

"""
        
        # Configuration details
        metadata = results.get("metadata", {})
        technical_content += f"""
## Configuration

- **Benchmarks:** {', '.join(metadata.get('benchmarks', []))}
- **Models:** {', '.join(metadata.get('models', []))}
- **Timestamp:** {metadata.get('timestamp', 'N/A')}

## Reproducibility

All evaluations can be reproduced using:
```bash
python src/evaluation/run_evaluation.py --benchmarks {' '.join(metadata.get('benchmarks', []))} --models {' '.join(metadata.get('models', []))}
```

---
*Technical Team - HierRAGMed*
"""
        
        # Save technical report
        tech_file = output_dir / f"technical_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(tech_file, 'w', encoding='utf-8') as f:
            f.write(technical_content)
        
        return str(tech_file)
    
    def generate_comparison_report(self, comparison_results: Dict, output_dir: Optional[Path] = None) -> str:
        """Generate model comparison report."""
        if output_dir is None:
            output_dir = self.results_dir / "comparative"
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        comparison_content = f"""# Model Comparison Report

**Generated:** {timestamp}

## Comparison Summary

"""
        
        summary = comparison_results.get("summary", {})
        comparison_content += f"""
- **Overall Winner:** {summary.get('overall_winner', 'Unknown')}
- **Total Benchmarks:** {summary.get('total_benchmarks', 0)}
- **Significant Differences:** {summary.get('significant_differences_count', 0)}

### Benchmark Winners
"""
        
        benchmark_wins = summary.get("benchmark_wins", {})
        for model, wins in benchmark_wins.items():
            comparison_content += f"- **{model}:** {wins} benchmark(s)\n"
        
        # Key findings
        key_findings = summary.get("key_findings", [])
        if key_findings:
            comparison_content += "\n### Key Findings\n\n"
            for finding in key_findings:
                comparison_content += f"- {finding}\n"
        
        comparison_content += """

---
*Generated by HierRAGMed Comparative Evaluator*
"""
        
        # Save comparison report
        comp_file = output_dir / f"model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(comp_file, 'w', encoding='utf-8') as f:
            f.write(comparison_content)
        
        return str(comp_file)