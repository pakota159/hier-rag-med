#!/usr/bin/env python3
"""
Complete Hierarchical System Debug Script
File: debug_hierarchical_system.py

Tests every component to identify performance issues.
Run this to find exactly where the problem is.
"""

import sys
import json
import traceback
from pathlib import Path

# Add project to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

def test_1_foundation_data():
    """Test 1: Check foundation data quality and sources."""
    print("=" * 60)
    print("ğŸ” TEST 1: Foundation Data Analysis")
    print("=" * 60)
    
    try:
        foundation_files = [
            Path('data/foundation_dataset/foundation_medical_data.json'),
            Path('data/foundation/foundation_medical_data.json')
        ]
        
        for f in foundation_files:
            if f.exists():
                print(f"ğŸ“ Found foundation data: {f}")
                with open(f) as file:
                    data = json.load(file)
                
                print(f"ğŸ“Š Total documents: {len(data)}")
                
                # Analyze sources
                sources = {}
                therapeutic_count = 0
                for doc in data:
                    source = doc['metadata'].get('source', 'unknown')
                    sources[source] = sources.get(source, 0) + 1
                    
                    # Check for therapeutic content
                    text = doc.get('text', '').lower()
                    if any(word in text for word in ['benefit', 'reduce', 'improve', 'prevent', 'effective']):
                        therapeutic_count += 1
                
                print("ğŸ”¬ Data sources:")
                for source, count in sources.items():
                    print(f"   {source}: {count} docs")
                
                therapeutic_ratio = therapeutic_count / len(data)
                print(f"ğŸ“ˆ Therapeutic content: {therapeutic_count}/{len(data)} ({therapeutic_ratio*100:.1f}%)")
                
                if therapeutic_ratio > 0.7:
                    print("âœ… STRONG therapeutic focus detected")
                elif therapeutic_ratio > 0.3:
                    print("ğŸ”„ MODERATE therapeutic focus detected")
                else:
                    print("âŒ LOW therapeutic focus - mostly exam/danger focused")
                
                return True, len(data), therapeutic_ratio
        
        print("âŒ No foundation data found")
        return False, 0, 0
        
    except Exception as e:
        print(f"âŒ Foundation data test failed: {e}")
        traceback.print_exc()
        return False, 0, 0


def test_2_collections():
    """Test 2: Check if hierarchical collections are working."""
    print("\n" + "=" * 60)
    print("ğŸ” TEST 2: Hierarchical Collections")
    print("=" * 60)
    
    try:
        from basic_reasoning.config import Config
        from basic_reasoning.retrieval import HierarchicalRetriever
        
        config = Config()
        retriever = HierarchicalRetriever(config)
        
        # Load collections
        retriever.load_hierarchical_collections()
        print("âœ… Collections loaded successfully")
        
        # Test search
        test_queries = [
            "diabetes treatment metformin",
            "medical ethics disclosure",
            "research methodology",
            "clinical procedure guidelines"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” Testing query: '{query}'")
            results = retriever.hierarchical_search(query)
            
            total_results = sum(len(docs) for docs in results.values())
            print(f"   Total retrieved: {total_results} documents")
            
            for tier_name, docs in results.items():
                print(f"   {tier_name}: {len(docs)} docs")
                if docs:
                    # Analyze retrieved content
                    sample_doc = docs[0]
                    content = sample_doc.get('content', sample_doc.get('text', ''))[:150]
                    source = sample_doc.get('metadata', {}).get('source', 'unknown')
                    print(f"      Sample: {content}...")
                    print(f"      Source: {source}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Collections test failed: {e}")
        traceback.print_exc()
        return False


def test_3_generation():
    """Test 3: Check generation with different methods."""
    print("\n" + "=" * 60)
    print("ğŸ” TEST 3: Generation Testing")
    print("=" * 60)
    
    try:
        from basic_reasoning.config import Config
        from basic_reasoning.generation import HierarchicalGenerator
        import inspect
        
        config = Config()
        generator = HierarchicalGenerator(config)
        
        # Check available methods
        methods = [m for m in dir(generator) if not m.startswith('_') and callable(getattr(generator, m))]
        print(f"ğŸ“‹ Available methods: {methods}")
        
        # Check method signatures
        for method_name in ['generate', 'generate_hierarchical_response']:
            if hasattr(generator, method_name):
                method = getattr(generator, method_name)
                sig = inspect.signature(method)
                print(f"ğŸ” {method_name} signature: {sig}")
        
        # Test direct Ollama call
        print("\nğŸ¤– Testing direct Ollama call:")
        test_prompt = "Answer this multiple choice question. Choose A, B, C, or D.\nQuestion: What is the best first-line treatment for type 2 diabetes?\nA) Insulin\nB) Metformin\nC) Sulfonylureas\nD) Diet only\nAnswer:"
        
        try:
            response = generator._call_ollama(test_prompt)
            print(f"   Raw response: '{response}'")
            print(f"   Response length: {len(response)} chars")
            
            # Check if it's a valid letter answer
            clean_response = response.strip().upper()
            if clean_response in ['A', 'B', 'C', 'D']:
                print("âœ… Gives single letter answer")
            elif any(letter in clean_response for letter in ['A', 'B', 'C', 'D']):
                print("ğŸ”„ Contains letter but with extra text")
            else:
                print("âŒ No letter answer found")
            
            return True, response
            
        except Exception as e:
            print(f"âŒ Direct Ollama call failed: {e}")
            return False, str(e)
        
    except Exception as e:
        print(f"âŒ Generation test failed: {e}")
        traceback.print_exc()
        return False, str(e)


def test_4_mirage_questions():
    """Test 4: Analyze MIRAGE questions and test system on them."""
    print("\n" + "=" * 60)
    print("ğŸ” TEST 4: MIRAGE Question Analysis")
    print("=" * 60)
    
    try:
        # Load MIRAGE data
        mirage_file = Path('mirage/benchmark.json')
        if not mirage_file.exists():
            print(f"âŒ MIRAGE data not found at {mirage_file}")
            return False
        
        with open(mirage_file) as f:
            data = json.load(f)
        
        # Analyze question types
        question_types = {}
        total_questions = 0
        
        for dataset_name, questions in data.items():
            question_count = len(questions)
            question_types[dataset_name] = question_count
            total_questions += question_count
        
        print(f"ğŸ“Š MIRAGE composition:")
        for dataset, count in question_types.items():
            percentage = (count / total_questions) * 100
            print(f"   {dataset}: {count} questions ({percentage:.1f}%)")
        
        # Test on different question types
        print("\nğŸ§ª Testing system on sample questions:")
        
        try:
            from basic_reasoning.config import Config
            from basic_reasoning.retrieval import HierarchicalRetriever
            from basic_reasoning.generation import HierarchicalGenerator
            
            config = Config()
            retriever = HierarchicalRetriever(config)
            generator = HierarchicalGenerator(config)
            retriever.load_hierarchical_collections()
            
            # Test on one question from each type
            for dataset_name, questions in data.items():
                if not questions:
                    continue
                    
                first_question_id = list(questions.keys())[0]
                sample_question = questions[first_question_id]
                
                question_text = sample_question.get('question', '')
                expected_answer = sample_question.get('answer', '')
                options = sample_question.get('options', {})
                
                print(f"\nğŸ“‹ {dataset_name.upper()} question test:")
                print(f"   Question: {question_text[:100]}...")
                print(f"   Expected: {expected_answer}")
                
                # Test retrieval
                hierarchical_results = retriever.hierarchical_search(question_text)
                total_retrieved = sum(len(docs) for docs in hierarchical_results.values())
                print(f"   Retrieved: {total_retrieved} docs")
                
                # Check relevance of retrieved content
                if total_retrieved > 0:
                    sample_content = ""
                    for docs in hierarchical_results.values():
                        if docs:
                            sample_content = docs[0].get('content', docs[0].get('text', ''))
                            break
                    
                    # Check if retrieved content matches question type
                    content_lower = sample_content.lower()
                    if 'ethics' in question_text.lower() and 'benefit' in content_lower:
                        print("   âš ï¸ Ethics question but therapeutic content retrieved")
                    elif 'treatment' in question_text.lower() and 'benefit' in content_lower:
                        print("   âœ… Treatment question with therapeutic content")
                    else:
                        print("   ğŸ”„ Mixed relevance")
                
                # Test generation (try different methods)
                try:
                    # Method 1: Try with hierarchical results
                    if hasattr(generator, 'generate_hierarchical_response'):
                        sig = str(inspect.signature(generator.generate_hierarchical_response))
                        if 'hierarchical_results' in sig:
                            model_response = generator.generate_hierarchical_response(hierarchical_results, question_text)
                        else:
                            model_response = generator.generate_hierarchical_response(question_text)
                    else:
                        # Method 2: Try regular generate
                        context = []
                        for docs in hierarchical_results.values():
                            context.extend(docs)
                        model_response = generator.generate(question_text, context)
                    
                    print(f"   Model output: '{model_response.strip()}'")
                    
                    # Check accuracy
                    model_answer = model_response.strip().upper()
                    if model_answer == expected_answer:
                        print("   âœ… CORRECT")
                    elif expected_answer in model_answer:
                        print("   ğŸ”„ CONTAINS CORRECT")
                    else:
                        print("   âŒ WRONG")
                        
                except Exception as e:
                    print(f"   âŒ Generation failed: {e}")
                
                # Only test first few to avoid too much output
                break
            
            return True
            
        except Exception as e:
            print(f"âŒ System test on MIRAGE failed: {e}")
            traceback.print_exc()
            return False
        
    except Exception as e:
        print(f"âŒ MIRAGE analysis failed: {e}")
        traceback.print_exc()
        return False


def test_5_comparison():
    """Test 5: Compare with baseline/random performance."""
    print("\n" + "=" * 60)
    print("ğŸ” TEST 5: Performance Comparison")
    print("=" * 60)
    
    try:
        # Calculate expected random performance
        mirage_file = Path('mirage/benchmark.json')
        if mirage_file.exists():
            with open(mirage_file) as f:
                data = json.load(f)
            
            # Analyze option counts
            option_counts = {}
            for dataset_name, questions in data.items():
                for question_id, question in questions.items():
                    options = question.get('options', {})
                    num_options = len(options)
                    option_counts[num_options] = option_counts.get(num_options, 0) + 1
            
            print("ğŸ“Š MIRAGE option analysis:")
            total_questions = sum(option_counts.values())
            weighted_random = 0
            
            for num_options, count in option_counts.items():
                percentage = (count / total_questions) * 100
                random_chance = (1 / num_options) * 100 if num_options > 0 else 0
                weighted_random += random_chance * (count / total_questions)
                print(f"   {num_options} options: {count} questions ({percentage:.1f}%) - Random: {random_chance:.1f}%")
            
            print(f"\nğŸ“ˆ Expected random performance: {weighted_random:.1f}%")
            print(f"ğŸ“ˆ Your system performance: 40.7%")
            print(f"ğŸ“ˆ Performance vs random: {40.7 - weighted_random:.1f} percentage points")
            
            if 40.7 > weighted_random + 10:
                print("âœ… Significantly better than random")
            elif 40.7 > weighted_random:
                print("ğŸ”„ Better than random but not by much")
            else:
                print("âŒ Close to or worse than random")
            
            return True
        
        return False
        
    except Exception as e:
        print(f"âŒ Comparison test failed: {e}")
        return False


def main():
    """Run all diagnostic tests."""
    print("ğŸš€ Hierarchical System Complete Diagnostic")
    print("ğŸ¯ Goal: Find why MIRAGE performance is 40.7% instead of 70-75%")
    print("=" * 80)
    
    results = {}
    
    # Run all tests
    results['foundation'] = test_1_foundation_data()
    results['collections'] = test_2_collections()
    results['generation'] = test_3_generation()
    results['mirage'] = test_4_mirage_questions()
    results['comparison'] = test_5_comparison()
    
    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“‹ DIAGNOSTIC SUMMARY")
    print("=" * 80)
    
    foundation_ok, doc_count, therapeutic_ratio = results['foundation'] if isinstance(results['foundation'], tuple) else (results['foundation'], 0, 0)
    generation_ok, sample_response = results['generation'] if isinstance(results['generation'], tuple) else (results['generation'], "")
    
    print(f"ğŸ“Š Foundation Data: {'âœ… OK' if foundation_ok else 'âŒ ISSUE'} ({doc_count} docs, {therapeutic_ratio*100:.1f}% therapeutic)")
    print(f"ğŸ” Collections: {'âœ… OK' if results['collections'] else 'âŒ ISSUE'}")
    print(f"ğŸ¤– Generation: {'âœ… OK' if generation_ok else 'âŒ ISSUE'}")
    print(f"ğŸ“‹ MIRAGE Compat: {'âœ… OK' if results['mirage'] else 'âŒ ISSUE'}")
    print(f"ğŸ“ˆ Comparison: {'âœ… OK' if results['comparison'] else 'âŒ ISSUE'}")
    
    # Diagnosis
    print("\nğŸ” LIKELY ISSUES:")
    
    if not foundation_ok:
        print("âŒ Foundation data missing or corrupted")
    elif therapeutic_ratio < 0.3:
        print("âŒ Foundation data not therapeutic enough for good performance")
    
    if not results['collections']:
        print("âŒ Hierarchical retrieval not working properly")
    
    if not generation_ok:
        print("âŒ Generation system has errors")
    elif "A" not in str(sample_response) and "B" not in str(sample_response):
        print("âŒ Model not giving letter answers as expected by MIRAGE")
    
    if not results['mirage']:
        print("âŒ System not compatible with MIRAGE question format")
    
    # Recommendations
    print("\nğŸš€ RECOMMENDATIONS:")
    
    if therapeutic_ratio > 0.7 and results['collections'] and generation_ok:
        print("ğŸ’¡ System components work - issue likely in prompt engineering or MIRAGE format mismatch")
        print("ğŸ”§ Try: Improve prompts to handle diverse MIRAGE question types (ethics, research, procedures)")
    elif not foundation_ok:
        print("ğŸ”§ Fix: Recreate foundation data with proper therapeutic guidelines")
    elif therapeutic_ratio < 0.5:
        print("ğŸ”§ Fix: Add more diverse medical guidelines (ethics, procedures, research)")
    else:
        print("ğŸ”§ Fix: Debug individual component issues found above")
    
    print("\n" + "=" * 80)
    print("âœ… Diagnostic complete! Check issues and recommendations above.")


if __name__ == "__main__":
    main()